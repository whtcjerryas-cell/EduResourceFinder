#!/usr/bin/env python3
"""
æµ‹è¯•è§†é¢‘è¯„ä¼°æµç¨‹å¹¶æ”¶é›† Token æ¶ˆè€—
å°†ç»“æœä¿å­˜åˆ° Excel è¡¨æ ¼
"""

import os
import sys
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

try:
    import pandas as pd
    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False
    print("âš ï¸  pandas æœªå®‰è£…ï¼Œæ— æ³•ä¿å­˜ Excelã€‚è¯·è¿è¡Œ: pip install pandas openpyxl")

from core.video_processor import VideoCrawler
from core.video_evaluator import VideoEvaluator
from logger_utils import get_logger

logger = get_logger('test_video_evaluation')

# Token ä½¿ç”¨è¿½è¸ªå™¨
class TokenTracker:
    """Token ä½¿ç”¨æƒ…å†µè¿½è¸ªå™¨"""
    
    def __init__(self):
        self.records = []
    
    def add_record(self, step: str, model: str, usage: Dict[str, Any], api_type: str = "text"):
        """
        æ·»åŠ  Token ä½¿ç”¨è®°å½•
        
        Args:
            step: æ­¥éª¤åç§°ï¼ˆå¦‚ "è§†è§‰åˆ†æ", "å†…å®¹ç›¸å…³åº¦è¯„ä¼°"ï¼‰
            model: æ¨¡å‹åç§°
            usage: Token ä½¿ç”¨æƒ…å†µå­—å…¸ï¼ˆåŒ…å« prompt_tokens, completion_tokens, total_tokensï¼‰
            api_type: API ç±»å‹ï¼ˆ"text" æˆ– "vision"ï¼‰
        """
        record = {
            "step": step,
            "model": model,
            "api_type": api_type,
            "prompt_tokens": usage.get("prompt_tokens", 0),
            "completion_tokens": usage.get("completion_tokens", 0),
            "total_tokens": usage.get("total_tokens", 0),
            "timestamp": datetime.now().isoformat()
        }
        self.records.append(record)
        logger.info(f"ğŸ“Š Tokenè®°å½•: {step} - {model} - æ€»è®¡: {record['total_tokens']} tokens")
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–æ±‡æ€»ç»Ÿè®¡"""
        if not self.records:
            return {
                "total_records": 0,
                "total_tokens": 0,
                "total_prompt_tokens": 0,
                "total_completion_tokens": 0,
                "by_model": {},
                "by_step": {}
            }
        
        total_tokens = sum(r["total_tokens"] for r in self.records)
        total_prompt = sum(r["prompt_tokens"] for r in self.records)
        total_completion = sum(r["completion_tokens"] for r in self.records)
        
        # æŒ‰æ¨¡å‹ç»Ÿè®¡
        by_model = {}
        for record in self.records:
            model = record["model"]
            if model not in by_model:
                by_model[model] = {"total_tokens": 0, "count": 0}
            by_model[model]["total_tokens"] += record["total_tokens"]
            by_model[model]["count"] += 1
        
        # æŒ‰æ­¥éª¤ç»Ÿè®¡
        by_step = {}
        for record in self.records:
            step = record["step"]
            if step not in by_step:
                by_step[step] = {"total_tokens": 0, "count": 0}
            by_step[step]["total_tokens"] += record["total_tokens"]
            by_step[step]["count"] += 1
        
        return {
            "total_records": len(self.records),
            "total_tokens": total_tokens,
            "total_prompt_tokens": total_prompt,
            "total_completion_tokens": total_completion,
            "by_model": by_model,
            "by_step": by_step
        }


# åŒ…è£… VideoEvaluator ä»¥è¿½è¸ª Token
class TokenTrackingVideoEvaluator(VideoEvaluator):
    """å¸¦ Token è¿½è¸ªçš„è§†é¢‘è¯„ä¼°å™¨"""
    
    def __init__(self, token_tracker: TokenTracker, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.token_tracker = token_tracker
    
    def _analyze_frame_design(self, frames_paths: List[str]) -> Dict[str, Any]:
        """é‡å†™ä»¥è¿½è¸ª Token"""
        result = super()._analyze_frame_design(frames_paths)
        
        # å¦‚æœç»“æœä¸­åŒ…å« token_usageï¼Œè®°å½•åˆ°è¿½è¸ªå™¨
        if "token_usage" in result:
            usage = result["token_usage"]
            self.token_tracker.add_record(
                step="è§†è§‰åˆ†æ",
                model="gpt-4o",
                usage=usage,
                api_type="vision"
            )
        
        return result
    
    def _evaluate_relevance(self, transcript: Optional[str], knowledge_point: Optional[Dict[str, Any]], video_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """é‡å†™ä»¥è¿½è¸ª Token"""
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        result = super()._evaluate_relevance(transcript, knowledge_point, video_metadata)
        
        # æ³¨æ„ï¼šAIBuildersClient çš„ call_llm æ–¹æ³•æ²¡æœ‰è¿”å› usage
        # æš‚æ—¶æ‰‹åŠ¨æ·»åŠ ä¸€ä¸ªä¼°ç®—å€¼ï¼ˆåŸºäºå®é™…è°ƒç”¨ï¼‰
        # å®é™…åº”è¯¥ä¿®æ”¹ AIBuildersClient è¿”å› usage
        
        return result
    
    def _evaluate_pedagogy(self, transcript: Optional[str], video_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """é‡å†™ä»¥è¿½è¸ª Token"""
        result = super()._evaluate_pedagogy(transcript, video_metadata)
        
        # åŒä¸Šï¼Œæš‚æ—¶è·³è¿‡
        
        return result


def test_video_evaluation(video_path: str, output_dir: Optional[str] = None) -> Dict[str, Any]:
    """
    æµ‹è¯•è§†é¢‘è¯„ä¼°æµç¨‹
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        æµ‹è¯•ç»“æœå­—å…¸
    """
    print("="*80)
    print("ğŸ§ª æµ‹è¯•è§†é¢‘è¯„ä¼°æµç¨‹")
    print("="*80)
    
    video_path = Path(video_path)
    if not video_path.exists():
        raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
    
    print(f"\nğŸ“¹ æµ‹è¯•è§†é¢‘: {video_path.name}")
    print(f"   è·¯å¾„: {video_path}")
    print(f"   å¤§å°: {video_path.stat().st_size / 1024 / 1024:.2f} MB")
    
    # åˆå§‹åŒ– Token è¿½è¸ªå™¨
    token_tracker = TokenTracker()
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if output_dir is None:
        output_dir = project_root / "data" / "videos" / "analyzed"
    else:
        output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # æ­¥éª¤1: å¤„ç†è§†é¢‘ï¼ˆæå–å…³é”®å¸§ç­‰ï¼‰
    print(f"\n{'='*80}")
    print("[æ­¥éª¤ 1/4] â³ å¤„ç†è§†é¢‘ï¼ˆæå–å…³é”®å¸§ã€éŸ³é¢‘ç­‰ï¼‰...")
    print(f"{'='*80}")
    
    video_crawler = VideoCrawler()
    
    # å¯¹äºæœ¬åœ°æ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å¤„ç†
    # ä½¿ç”¨ ffmpeg-python æå–å…ƒæ•°æ®å’Œå…³é”®å¸§
    try:
        import ffmpeg
        # éªŒè¯ ffmpeg-python æ˜¯å¦æ­£ç¡®å®‰è£…ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰ probe æ–¹æ³•ï¼‰
        if not hasattr(ffmpeg, 'probe'):
            # å¯èƒ½æ˜¯å®‰è£…äº†é”™è¯¯çš„ ffmpeg åŒ…ï¼Œå°è¯•é‡æ–°å®‰è£…
            raise ImportError(
                "ffmpeg-python åŒ…ä¸æ­£ç¡®ã€‚è¯·è¿è¡Œ:\n"
                "  pip uninstall ffmpeg\n"
                "  pip install ffmpeg-python"
            )
    except ImportError as e:
        raise ImportError(
            f"ffmpeg-python æœªæ­£ç¡®å®‰è£…ã€‚è¯·è¿è¡Œ: pip install ffmpeg-python\n"
            f"é”™è¯¯è¯¦æƒ…: {str(e)}\n"
            f"æ³¨æ„ï¼šè¿˜éœ€è¦å®‰è£… ffmpeg äºŒè¿›åˆ¶æ–‡ä»¶"
        )
    
    # è·å–è§†é¢‘å…ƒæ•°æ®
    try:
        probe = ffmpeg.probe(str(video_path))
        video_stream = next((stream for stream in probe['streams'] if stream['codec_type'] == 'video'), None)
        
        if not video_stream:
            raise Exception("æ— æ³•æ‰¾åˆ°è§†é¢‘æµ")
        
        # æå–åˆ†è¾¨ç‡
        width = int(video_stream.get('width', 0))
        height = int(video_stream.get('height', 0))
        duration = float(probe.get('format', {}).get('duration', 0))
        
        video_metadata = {
            "title": video_path.stem,
            "width": width,
            "height": height,
            "max_resolution_height": height,  # æœ¬åœ°æ–‡ä»¶ï¼Œä½¿ç”¨å®é™…é«˜åº¦
            "duration": duration,
            "format": probe.get('format', {}).get('format_name', 'unknown')
        }
        
        print(f"âœ… è§†é¢‘å…ƒæ•°æ®æå–æˆåŠŸ")
        print(f"   åˆ†è¾¨ç‡: {width}x{height}")
        print(f"   æ—¶é•¿: {duration:.2f} ç§’")
        
    except Exception as e:
        raise Exception(f"æå–è§†é¢‘å…ƒæ•°æ®å¤±è´¥: {str(e)}")
    
    # æå–å…³é”®å¸§
    frames_paths = []
    num_frames = 6
    
    try:
        video_stem = video_path.stem
        for i in range(num_frames):
            timestamp = duration * i / (num_frames - 1) if num_frames > 1 else duration / 2
            frame_filename = f"{video_stem}_frame_{i+1:02d}.jpg"
            frame_path = output_dir / frame_filename
            
            try:
                stream = ffmpeg.input(str(video_path), ss=timestamp)
                stream = ffmpeg.output(stream, str(frame_path), vframes=1, **{'qscale:v': 2})
                ffmpeg.run(stream, overwrite_output=True, quiet=True)
                
                if frame_path.exists():
                    frames_paths.append(str(frame_path))
                    print(f"  âœ… å¸§ {i+1}/{num_frames}: {frame_filename}")
                else:
                    print(f"  âš ï¸  å¸§ {i+1}/{num_frames} æœªç”Ÿæˆ")
            except Exception as e:
                print(f"  âš ï¸  å¸§ {i+1}/{num_frames} æå–å¤±è´¥: {str(e)}")
        
        print(f"âœ… å…³é”®å¸§æå–å®Œæˆ: {len(frames_paths)}/{num_frames} å¼ ")
        
    except Exception as e:
        print(f"âš ï¸  å…³é”®å¸§æå–å¤±è´¥: {str(e)}")
        frames_paths = []
    
    # æå–éŸ³é¢‘ï¼ˆå¯é€‰ï¼‰
    audio_path = None
    try:
        audio_filename = video_path.stem + ".mp3"
        audio_path = output_dir / audio_filename
        
        stream = ffmpeg.input(str(video_path))
        stream = ffmpeg.output(stream, str(audio_path), acodec='libmp3lame', ac=1, ar='22050')
        ffmpeg.run(stream, overwrite_output=True, quiet=True)
        
        if audio_path.exists():
            print(f"âœ… éŸ³é¢‘æå–æˆåŠŸ: {audio_filename}")
        else:
            audio_path = None
            print(f"âš ï¸  éŸ³é¢‘æå–å¤±è´¥")
    except Exception as e:
        print(f"âš ï¸  éŸ³é¢‘æå–å¤±è´¥: {str(e)}")
        audio_path = None
    
    print(f"âœ… è§†é¢‘å¤„ç†å®Œæˆ")
    print(f"   æœ€å¤§åˆ†è¾¨ç‡: {video_metadata.get('max_resolution_height', 'N/A')}p")
    print(f"   å…³é”®å¸§æ•°é‡: {len(frames_paths)}")
    if frames_paths:
        print(f"   å…³é”®å¸§è·¯å¾„: {frames_paths[0]} ... ({len(frames_paths)} å¼ )")
    
    # æ­¥éª¤2: åˆå§‹åŒ–è¯„ä¼°å™¨ï¼ˆå¸¦ Token è¿½è¸ªï¼‰
    print(f"\n{'='*80}")
    print("[æ­¥éª¤ 2/4] ğŸ”§ åˆå§‹åŒ–è¯„ä¼°å™¨...")
    print(f"{'='*80}")
    
    evaluator = TokenTrackingVideoEvaluator(token_tracker=token_tracker)
    
    # æ‰‹åŠ¨è¿½è¸ªè§†è§‰ API çš„ Tokenï¼ˆå¦‚æœä½¿ç”¨ï¼‰
    # æˆ‘ä»¬éœ€è¦ä¿®æ”¹ vision_client çš„è°ƒç”¨ä»¥è¿”å› usage
    # æš‚æ—¶å…ˆè¿è¡Œè¯„ä¼°ï¼Œç„¶åæ‰‹åŠ¨æ·»åŠ è®°å½•
    
    print(f"âœ… è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # æ­¥éª¤3: è¿è¡Œè¯„ä¼°
    print(f"\n{'='*80}")
    print("[æ­¥éª¤ 3/4] ğŸ§  è¿è¡Œè§†é¢‘è¯„ä¼°...")
    print(f"{'='*80}")
    
    # æ¨¡æ‹ŸçŸ¥è¯†ç‚¹ä¿¡æ¯
    knowledge_point = {
        "topic_title_cn": "æ•°å­—è®¡æ•°",
        "learning_objective": "å­¦ä¹ æ•°å­— 141-160 çš„è®¡æ•°æ–¹æ³•"
    }
    
    evaluation_result = evaluator.evaluate_video_content(
        video_metadata=video_metadata,
        video_path=str(video_path),
        frames_paths=frames_paths,
        audio_path=audio_path,
        transcript=None,  # æš‚æ—¶æ²¡æœ‰å­—å¹•
        knowledge_point=knowledge_point
    )
    
    print(f"âœ… è¯„ä¼°å®Œæˆ")
    print(f"   æ€»åˆ†: {evaluation_result['overall_score']:.2f}/10")
    print(f"   è§†è§‰è´¨é‡: {evaluation_result['visual_quality']['combined_score']:.2f}/10")
    print(f"   å†…å®¹ç›¸å…³åº¦: {evaluation_result['relevance']['score']:.2f}/10")
    print(f"   æ•™å­¦è´¨é‡: {evaluation_result['pedagogy']['score']:.2f}/10")
    print(f"   çƒ­åº¦/å…ƒæ•°æ®: {evaluation_result['metadata']['score']:.2f}/10")
    
    # æ­¥éª¤4: æ”¶é›† Token ä½¿ç”¨æƒ…å†µ
    # æ³¨æ„ï¼šç”±äºåº•å±‚å®¢æˆ·ç«¯æ²¡æœ‰è¿”å› usageï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹ä»£ç 
    # è¿™é‡Œå…ˆåˆ›å»ºä¸€ä¸ªå ä½ç¬¦ï¼Œå®é™…éœ€è¦ä¿®æ”¹ AIBuildersClient å’Œ VisionClient
    
    print(f"\n{'='*80}")
    print("[æ­¥éª¤ 4/4] ğŸ“Š æ”¶é›† Token ä½¿ç”¨æƒ…å†µ...")
    print(f"{'='*80}")
    
    # è·å–æ±‡æ€»
    summary = token_tracker.get_summary()
    
    print(f"\nğŸ“Š Token ä½¿ç”¨æ±‡æ€»:")
    print(f"   æ€»è®°å½•æ•°: {summary['total_records']}")
    print(f"   æ€» Token: {summary['total_tokens']}")
    print(f"   è¾“å…¥ Token: {summary['total_prompt_tokens']}")
    print(f"   è¾“å‡º Token: {summary['total_completion_tokens']}")
    
    if summary['by_model']:
        print(f"\n   æŒ‰æ¨¡å‹ç»Ÿè®¡:")
        for model, stats in summary['by_model'].items():
            print(f"     {model}: {stats['total_tokens']} tokens ({stats['count']} æ¬¡è°ƒç”¨)")
    
    if summary['by_step']:
        print(f"\n   æŒ‰æ­¥éª¤ç»Ÿè®¡:")
        for step, stats in summary['by_step'].items():
            print(f"     {step}: {stats['total_tokens']} tokens ({stats['count']} æ¬¡è°ƒç”¨)")
    
    # å‡†å¤‡ç»“æœ
    result = {
        "video_path": str(video_path),
        "video_name": video_path.name,
        "evaluation_result": evaluation_result,
        "token_usage": {
            "records": token_tracker.records,
            "summary": summary
        },
        "metadata": video_metadata,
        "frames_count": len(frames_paths),
        "timestamp": datetime.now().isoformat()
    }
    
    return result


def save_to_excel(result: Dict[str, Any], output_file: str):
    """
    å°†ç»“æœä¿å­˜åˆ° Excel
    
    Args:
        result: æµ‹è¯•ç»“æœå­—å…¸
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    if not HAS_PANDAS:
        print("âš ï¸  pandas æœªå®‰è£…ï¼Œæ— æ³•ä¿å­˜ Excel")
        return
    
    print(f"\n{'='*80}")
    print(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ° Excel...")
    print(f"{'='*80}")
    
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»º Excel writer
    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        # Sheet 1: è¯„ä¼°ç»“æœæ‘˜è¦
        summary_data = {
            "æŒ‡æ ‡": [
                "æ€»åˆ†",
                "è§†è§‰è´¨é‡ï¼ˆç¡¬æŒ‡æ ‡ï¼‰",
                "è§†è§‰è´¨é‡ï¼ˆè½¯æŒ‡æ ‡ï¼‰",
                "è§†è§‰è´¨é‡ï¼ˆç»¼åˆï¼‰",
                "å†…å®¹ç›¸å…³åº¦",
                "æ•™å­¦è´¨é‡",
                "çƒ­åº¦/å…ƒæ•°æ®"
            ],
            "åˆ†æ•°": [
                result["evaluation_result"]["overall_score"],
                result["evaluation_result"]["visual_quality"]["tech_score"],
                result["evaluation_result"]["visual_quality"]["design_score"],
                result["evaluation_result"]["visual_quality"]["combined_score"],
                result["evaluation_result"]["relevance"]["score"],
                result["evaluation_result"]["pedagogy"]["score"],
                result["evaluation_result"]["metadata"]["score"]
            ],
            "æƒé‡": [
                "100%",
                "10%",
                "10%",
                "20%",
                "40%",
                "30%",
                "10%"
            ]
        }
        df_summary = pd.DataFrame(summary_data)
        df_summary.to_excel(writer, sheet_name="è¯„ä¼°ç»“æœ", index=False)
        
        # Sheet 2: Token ä½¿ç”¨è¯¦æƒ…
        if result["token_usage"]["records"]:
            df_tokens = pd.DataFrame(result["token_usage"]["records"])
            df_tokens.to_excel(writer, sheet_name="Tokenä½¿ç”¨è¯¦æƒ…", index=False)
        else:
            # å¦‚æœæ²¡æœ‰è®°å½•ï¼Œåˆ›å»ºä¸€ä¸ªç©ºè¡¨
            df_tokens = pd.DataFrame(columns=["step", "model", "api_type", "prompt_tokens", "completion_tokens", "total_tokens", "timestamp"])
            df_tokens.to_excel(writer, sheet_name="Tokenä½¿ç”¨è¯¦æƒ…", index=False)
        
        # Sheet 3: Token æ±‡æ€»ç»Ÿè®¡
        summary = result["token_usage"]["summary"]
        summary_data = {
            "ç»Ÿè®¡é¡¹": [
                "æ€»è®°å½•æ•°",
                "æ€» Token",
                "è¾“å…¥ Token",
                "è¾“å‡º Token"
            ],
            "æ•°å€¼": [
                summary["total_records"],
                summary["total_tokens"],
                summary["total_prompt_tokens"],
                summary["total_completion_tokens"]
            ]
        }
        df_summary_tokens = pd.DataFrame(summary_data)
        df_summary_tokens.to_excel(writer, sheet_name="Tokenæ±‡æ€»", index=False)
        
        # Sheet 4: è§†é¢‘å…ƒæ•°æ®
        metadata = result["metadata"]
        metadata_data = {
            "å­—æ®µ": list(metadata.keys()),
            "å€¼": [str(v) for v in metadata.values()]
        }
        df_metadata = pd.DataFrame(metadata_data)
        df_metadata.to_excel(writer, sheet_name="è§†é¢‘å…ƒæ•°æ®", index=False)
    
    print(f"âœ… Excel æ–‡ä»¶å·²ä¿å­˜: {output_path}")
    print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024:.2f} KB")


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
    missing = []
    
    try:
        import ffmpeg
        # æ£€æŸ¥ ffmpeg äºŒè¿›åˆ¶æ–‡ä»¶
        import subprocess
        try:
            subprocess.run(['ffprobe', '-version'], capture_output=True, check=True)
        except (subprocess.CalledProcessError, FileNotFoundError):
            missing.append("ffmpeg äºŒè¿›åˆ¶æ–‡ä»¶ï¼ˆè¯·è¿è¡Œ: brew install ffmpegï¼‰")
    except ImportError:
        missing.append("ffmpeg-pythonï¼ˆè¯·è¿è¡Œ: pip install ffmpeg-pythonï¼‰")
    
    if not HAS_PANDAS:
        missing.append("pandasï¼ˆè¯·è¿è¡Œ: pip install pandas openpyxlï¼‰")
    
    if missing:
        print("âŒ ç¼ºå°‘ä»¥ä¸‹ä¾èµ–:")
        for item in missing:
            print(f"   - {item}")
        print("\nğŸ’¡ å®‰è£…å‘½ä»¤:")
        print("   pip install ffmpeg-python pandas openpyxl")
        print("   brew install ffmpeg")
        return False
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return
    
    # æŸ¥æ‰¾æµ‹è¯•è§†é¢‘
    videos_dir = project_root / "data" / "videos"
    video_files = list(videos_dir.glob("*.mp4"))
    
    if not video_files:
        print(f"âŒ æœªæ‰¾åˆ°æµ‹è¯•è§†é¢‘æ–‡ä»¶")
        print(f"ğŸ’¡ æç¤º: è¯·å°†æµ‹è¯•è§†é¢‘æ”¾åˆ° {videos_dir} ç›®å½•ä¸‹")
        return
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªè§†é¢‘
    test_video = video_files[0]
    print(f"ğŸ“¹ æ‰¾åˆ°æµ‹è¯•è§†é¢‘: {test_video.name}")
    
    try:
        # è¿è¡Œæµ‹è¯•
        result = test_video_evaluation(
            video_path=str(test_video),
            output_dir=str(project_root / "data" / "videos" / "analyzed")
        )
        
        # ä¿å­˜åˆ° Excel
        output_excel = project_root / "data" / "videos" / "test_result.xlsx"
        save_to_excel(result, str(output_excel))
        
        # ä¹Ÿä¿å­˜ JSON æ ¼å¼ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        output_json = project_root / "data" / "videos" / "test_result.json"
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… JSON ç»“æœå·²ä¿å­˜: {output_json}")
        
        print(f"\n{'='*80}")
        print("âœ… æµ‹è¯•å®Œæˆï¼")
        print(f"{'='*80}")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        import traceback
        print(f"\n\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        traceback.print_exc()

