#!/usr/bin/env python3
"""
åŸºäºLLMçš„æ™ºèƒ½æ¨èç†ç”±ç”Ÿæˆå™¨
ä¸ºæ¯ä¸ªæœç´¢ç»“æœç”Ÿæˆä¸ªæ€§åŒ–çš„æ¨èç†ç”±
"""

import sys
from pathlib import Path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from typing import Dict, List, Any, Optional
from logger_utils import get_logger

logger = get_logger('recommendation_generator')


class LLMRecommendationGenerator:
    """åŸºäºLLMçš„æ¨èç†ç”±ç”Ÿæˆå™¨"""

    def __init__(self, country_code: str = None):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨

        Args:
            country_code: å›½å®¶ä»£ç ï¼Œç”¨äºåŠ è½½çŸ¥è¯†åº“
        """
        self.country_code = country_code
        self.kb_manager = None
        self.llm_client = None

        # å¦‚æœæä¾›äº†country_codeï¼Œåˆå§‹åŒ–çŸ¥è¯†åº“ç®¡ç†å™¨
        if country_code:
            try:
                from core.knowledge_base_manager import get_knowledge_base_manager
                self.kb_manager = get_knowledge_base_manager(country_code)
                logger.info(f"[âœ… æ¨èç”Ÿæˆå™¨] å·²åŠ è½½ {country_code} çŸ¥è¯†åº“")
            except Exception as e:
                logger.warning(f"[âš ï¸ æ¨èç”Ÿæˆå™¨] çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {str(e)}")

        self._init_llm_client()

    def _init_llm_client(self):
        """å»¶è¿Ÿåˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            from llm_client import get_llm_client
            self.llm_client = get_llm_client()
            logger.info("âœ… æ¨èç†ç”±ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")

    def generate_recommendations_batch(
        self,
        results: List[Dict[str, Any]],
        query: str,
        metadata: Optional[Dict] = None
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡ç”Ÿæˆæ¨èç†ç”±ï¼ˆä½¿ç”¨LLMï¼Œè¶…æ—¶åˆ™å›é€€åˆ°è§„åˆ™ï¼‰

        Args:
            results: æœç´¢ç»“æœåˆ—è¡¨
            query: æœç´¢æŸ¥è¯¢
            metadata: å…ƒæ•°æ®ï¼ˆå›½å®¶ã€å¹´çº§ã€å­¦ç§‘ï¼‰

        Returns:
            æ·»åŠ äº†æ¨èç†ç”±çš„ç»“æœåˆ—è¡¨
        """
        if not results:
            return results

        # å¦‚æœLLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œç›´æ¥ä½¿ç”¨è§„åˆ™ç”Ÿæˆ
        if not self.llm_client:
            logger.info(f"[æ¨èç†ç”±ç”Ÿæˆ] LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨è§„åˆ™ç”Ÿæˆ")
            return self._fallback_to_rules(results, query, metadata)

        # å°è¯•ä½¿ç”¨LLMç”Ÿæˆæ¨èç†ç”±
        try:
            logger.info(f"[æ¨èç†ç”±ç”Ÿæˆ] æ­£åœ¨ä½¿ç”¨LLMç”Ÿæˆ {len(results)} ä¸ªç»“æœçš„æ¨èç†ç”±")

            # æ„å»ºæç¤ºè¯
            prompt = self._build_batch_prompt(results, query, metadata)

            # è·å–æ¨èæ¨¡å‹é…ç½®ï¼ˆä½¿ç”¨ gemini-2.5-pro æå‡è´¨é‡ï¼‰
            from core.config_loader import get_config
            config = get_config()
            models = config.get_llm_models()
            recommendation_model = 'gemini-2.5-pro'  # ğŸ”¥ ä½¿ç”¨æ›´é«˜è´¨é‡çš„æ¨¡å‹

            logger.info(f"[æ¨èç†ç”±ç”Ÿæˆ] ä½¿ç”¨æ¨èæ¨¡å‹: {recommendation_model}")

            # è°ƒç”¨LLMï¼ˆè¶…æ—¶æ§åˆ¶åœ¨å®¢æˆ·ç«¯å†…éƒ¨å¤„ç†ï¼‰
            import concurrent.futures
            import time

            # ğŸ“Š è®°å½•LLMè°ƒç”¨å¼€å§‹
            llm_start = time.time()

            def call_llm():
                response = self.llm_client.call_llm(
                    prompt=prompt,
                    max_tokens=150 * len(results),  # æ¯ä¸ªç»“æœçº¦150å­—
                    temperature=0.7,
                    model=recommendation_model  # ğŸ”¥ ä½¿ç”¨ gemini-2.5-pro
                )

                # ğŸ“Š è®°å½•LLMè°ƒç”¨ç»“æŸ
                llm_elapsed = time.time() - llm_start
                try:
                    from core.search_log_collector import get_log_collector
                    log_collector = get_log_collector()
                    if log_collector.current_log:
                        # ğŸ”¥ ä¸æˆªæ–­promptå’Œresponse
                        log_collector.record_llm_call(
                            model_name=recommendation_model,
                            function="æ¨èç†ç”±ç”Ÿæˆ",
                            provider="Internal API",
                            prompt=prompt,  # ğŸ”¥ å®Œæ•´æç¤ºè¯
                            input_data=f"ç»“æœæ•°é‡: {len(results)}, æŸ¥è¯¢: {query}",
                            output_data=response,  # ğŸ”¥ å®Œæ•´è¾“å‡º
                            execution_time=llm_elapsed
                        )
                        logger.debug(f"[ğŸ“Š æ—¥å¿—] LLMè°ƒç”¨å·²è®°å½•: {fast_model}, åŠŸèƒ½=æ¨èç†ç”±ç”Ÿæˆ, è€—æ—¶={llm_elapsed:.2f}ç§’")
                except Exception as e:
                    logger.warning(f"[ğŸ“Š æ—¥å¿—] è®°å½•LLMè°ƒç”¨å¤±è´¥: {str(e)}")

                return response

            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œï¼Œè®¾ç½®15ç§’è¶…æ—¶ï¼ˆå¿«é€Ÿæ¨¡å‹2-3ç§’ï¼Œç•™è¶³ä½™é‡ï¼‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(call_llm)
                try:
                    response = future.result(timeout=15)
                    logger.info(f"[æ¨èç†ç”±ç”Ÿæˆ] LLMè°ƒç”¨æˆåŠŸ")
                except concurrent.futures.TimeoutError:
                    logger.warning(f"[æ¨èç†ç”±ç”Ÿæˆ] LLMè°ƒç”¨è¶…æ—¶ï¼ˆ15ç§’ï¼‰ï¼Œå›é€€åˆ°è§„åˆ™ç”Ÿæˆ")
                    future.cancel()
                    return self._fallback_to_rules(results, query, metadata)

            # è§£æå“åº”
            recommendations = self._parse_batch_response(response, len(results))

            # æ·»åŠ æ¨èç†ç”±åˆ°ç»“æœä¸­
            for i, result in enumerate(results):
                if i < len(recommendations):
                    result['recommendation_reason'] = recommendations[i]
                else:
                    result['recommendation_reason'] = f"æ ¹æ®æœç´¢åŒ¹é…åº¦æ¨è"

            return results

        except TimeoutError as e:
            logger.warning(f"[æ¨èç†ç”±ç”Ÿæˆ] LLMè°ƒç”¨è¶…æ—¶: {str(e)}ï¼Œå›é€€åˆ°è§„åˆ™ç”Ÿæˆ")
            return self._fallback_to_rules(results, query, metadata)
        except Exception as e:
            logger.warning(f"[æ¨èç†ç”±ç”Ÿæˆ] LLMè°ƒç”¨å¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°è§„åˆ™ç”Ÿæˆ")
            return self._fallback_to_rules(results, query, metadata)

    def _build_batch_prompt(
        self,
        results: List[Dict[str, Any]],
        query: str,
        metadata: Optional[Dict]
    ) -> str:
        """æ„å»ºæ‰¹é‡ç”Ÿæˆæç¤ºè¯"""

        # è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_parts = []
        if metadata:
            country = metadata.get('country', '')
            grade = metadata.get('grade', '')
            subject = metadata.get('subject', '')

            if country:
                context_parts.append(f"å›½å®¶: {country}")
            if grade:
                context_parts.append(f"å¹´çº§: {grade}")
            if subject:
                context_parts.append(f"å­¦ç§‘: {subject}")

        context = "\n".join(context_parts) if context_parts else "é€šç”¨æ•™è‚²å†…å®¹æœç´¢"

        # ğŸ“š æ·»åŠ çŸ¥è¯†åº“å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
        knowledge_section = ""
        if self.kb_manager and self.kb_manager.knowledge:
            knowledge = self.kb_manager.knowledge

            # æ·»åŠ å¹´çº§è¡¨è¾¾
            if 'grade_expressions' in knowledge and knowledge['grade_expressions']:
                knowledge_section += "\n**ğŸ“š é‡è¦å¹´çº§è¡¨è¾¾ï¼ˆå¿…é¡»æ­£ç¡®è¯†åˆ«ï¼‰**:\n"
                for grade_key, grade_info in knowledge['grade_expressions'].items():
                    variants = grade_info.get('local_variants', [])
                    if variants:
                        variant_list = []
                        for v in variants:
                            if 'arabic' in v:
                                variant_list.append(f"{v['arabic']}")
                            elif 'english' in v:
                                note = f" ({v.get('note', '')})" if v.get('note') else ''
                                variant_list.append(f"{v['english']}{note}")

                        knowledge_section += f"- {grade_key}: {', '.join(variant_list)}\n"

                # æ·»åŠ å¸¸è§é”™è¯¯
                for grade_key, grade_info in knowledge['grade_expressions'].items():
                    mistakes = grade_info.get('common_mistakes', [])
                    if mistakes:
                        knowledge_section += f"\nâš ï¸ **{grade_key} å¸¸è§é”™è¯¯ï¼ˆå¿…é¡»é¿å…ï¼‰**:\n"
                        for m in mistakes:
                            knowledge_section += f"  â€¢ âŒ {m['mistake']}\n"
                            knowledge_section += f"  â€¢ âœ… {m['correction']}\n"

            # æ·»åŠ å­¦ç§‘å…³é”®è¯
            if 'subject_keywords' in knowledge and knowledge['subject_keywords']:
                knowledge_section += "\n**ğŸ“– å­¦ç§‘å…³é”®è¯è¡¨è¾¾**:\n"
                for subject_key, subject_info in knowledge['subject_keywords'].items():
                    variants = subject_info.get('local_variants', [])
                    if variants:
                        variant_list = []
                        for v in variants:
                            if 'arabic' in v:
                                variant_list.append(v['arabic'])
                            elif 'english' in v:
                                variant_list.append(v['english'])
                        knowledge_section += f"- {subject_key}: {', '.join(variant_list)}\n"

            # æ·»åŠ LLMå·²çŸ¥é—®é¢˜
            if 'llm_insights' in knowledge and knowledge['llm_insights']:
                insights = knowledge['llm_insights']
                if 'accuracy_issues' in insights and insights['accuracy_issues']:
                    # åªæ˜¾ç¤ºæœªä¿®å¤çš„é—®é¢˜
                    pending_issues = [i for i in insights['accuracy_issues']
                                    if i.get('status') != 'fixed']
                    if pending_issues:
                        knowledge_section += "\n**âš ï¸ å·²çŸ¥LLMè¯†åˆ«é—®é¢˜ï¼ˆå¿…é¡»æ³¨æ„ï¼‰**:\n"
                        for issue in pending_issues[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                            knowledge_section += f"â€¢ é—®é¢˜: {issue.get('issue', '')}\n"
                            knowledge_section += f"  ä¿®å¤: {issue.get('fix', '')}\n"

        # æ„å»ºç»“æœæ‘˜è¦ï¼ˆåªæ˜¾ç¤ºå‰10ä¸ªç»“æœï¼‰
        results_summary = []
        for i, result in enumerate(results[:10], 1):
            title = result.get('title', 'æœªçŸ¥æ ‡é¢˜')[:80]
            url = result.get('url', '')[:60]
            snippet = result.get('snippet', '')[:100]

            results_summary.append(
                f"{i}. æ ‡é¢˜: {title}\n"
                f"   URL: {url}\n"
                f"   æè¿°: {snippet}\n"
            )

        results_text = "\n".join(results_summary)

        if len(results) > 10:
            results_text += f"\n... è¿˜æœ‰ {len(results) - 10} ä¸ªç»“æœ\n"

        prompt = f"""è¯·ä¸ºä»¥ä¸‹æœç´¢ç»“æœç”Ÿæˆç®€æ´çš„æ¨èç†ç”±ï¼ˆæ¯æ¡20-50å­—ï¼‰ï¼š

**æœç´¢æŸ¥è¯¢**: {query}

**æœç´¢èƒŒæ™¯**:
{context}
{knowledge_section}

**æœç´¢ç»“æœ**:
{results_text}

**è¦æ±‚**:
1. ä¸ºæ¯ä¸ªç»“æœç”Ÿæˆ1æ¡æ¨èç†ç”±
2. æ¨èç†ç”±è¦å…·ä½“ã€ä¸ªæ€§åŒ–ï¼Œä¸è¦é›·åŒ
3. çªå‡ºæ¯ä¸ªç»“æœçš„ç‹¬ç‰¹ä¼˜åŠ¿
4. æ¯æ¡ç†ç”±20-50å­—
5. ä½¿ç”¨JSONæ•°ç»„æ ¼å¼è¿”å›
6. **å¿…é¡»æ­£ç¡®è¯†åˆ«å¹´çº§å’Œå­¦ç§‘è¡¨è¾¾**ï¼ˆå‚è€ƒä¸Šé¢çš„å¹´çº§è¡¨è¾¾å’Œå¸¸è§é”™è¯¯ï¼‰

**è¾“å‡ºæ ¼å¼**:
[
    "æ¨èç†ç”±1",
    "æ¨èç†ç”±2",
    ...
]

è¯·ç¡®ä¿è¾“å‡ºæ˜¯æœ‰æ•ˆçš„JSONæ•°ç»„æ ¼å¼ã€‚"""

        return prompt

    def _parse_batch_response(self, response: str, expected_count: int) -> List[str]:
        """è§£ææ‰¹é‡å“åº”"""
        import re
        import json

        try:
            # æå–JSONæ•°ç»„
            json_match = re.search(r'\[\s*".*"\s*\]', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                recommendations = json.loads(json_str)

                if isinstance(recommendations, list) and len(recommendations) == expected_count:
                    return recommendations

            # å¦‚æœè§£æå¤±è´¥æˆ–æ•°é‡ä¸åŒ¹é…ï¼Œä½¿ç”¨è§„åˆ™ç”Ÿæˆ
            logger.warning(f"[æ¨èç†ç”±ç”Ÿæˆ] LLMè¿”å›æ•°é‡ä¸åŒ¹é…ï¼Œä½¿ç”¨è§„åˆ™ç”Ÿæˆ")
            return [f"æ ¹æ®æœç´¢åŒ¹é…åº¦æ¨è"] * expected_count

        except Exception as e:
            logger.error(f"[æ¨èç†ç”±ç”Ÿæˆ] è§£æå¤±è´¥: {str(e)}")
            return [f"æ ¹æ®æœç´¢åŒ¹é…åº¦æ¨è"] * expected_count

    def _fallback_to_rules(
        self,
        results: List[Dict[str, Any]],
        query: str,
        metadata: Optional[Dict]
    ) -> List[Dict[str, Any]]:
        """å›é€€åˆ°è§„åˆ™ç”Ÿæˆ"""
        from core.result_scorer import IntelligentResultScorer

        scorer = IntelligentResultScorer()

        for result in results:
            score = result.get('score', 5.0)
            reason = scorer._generate_recommendation_reason(result, score)
            result['recommendation_reason'] = reason

        return results


# å…¨å±€å®ä¾‹å­—å…¸ï¼ˆæ”¯æŒå¤šä¸ªå›½å®¶çš„å®ä¾‹ï¼‰
_generator_instances: Dict[str, LLMRecommendationGenerator] = {}
_default_instance: Optional[LLMRecommendationGenerator] = None


def get_recommendation_generator(country_code: str = None) -> LLMRecommendationGenerator:
    """
    è·å–æ¨èç†ç”±ç”Ÿæˆå™¨å®ä¾‹

    Args:
        country_code: å›½å®¶ä»£ç ï¼Œç”¨äºè·å–å¸¦çŸ¥è¯†åº“çš„å®ä¾‹

    Returns:
        æ¨èç†ç”±ç”Ÿæˆå™¨å®ä¾‹
    """
    global _default_instance

    # å¦‚æœæä¾›äº†country_codeï¼Œè¿”å›å›½å®¶ç‰¹å®šçš„å®ä¾‹
    if country_code:
        country_key = country_code.upper()
        if country_key not in _generator_instances:
            _generator_instances[country_key] = LLMRecommendationGenerator(country_code)
            logger.info(f"[æ¨èç”Ÿæˆå™¨] åˆ›å»º {country_key} ä¸“ç”¨å®ä¾‹")
        return _generator_instances[country_key]

    # å¦åˆ™è¿”å›é»˜è®¤å®ä¾‹ï¼ˆä¸å¸¦çŸ¥è¯†åº“ï¼‰
    if _default_instance is None:
        _default_instance = LLMRecommendationGenerator()
    return _default_instance
