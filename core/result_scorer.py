#!/usr/bin/env python3
"""
æ™ºèƒ½ç»“æœè¯„åˆ†æ¨¡å— - çº¯LLMè¯„ä¼°ç‰ˆæœ¬
åªä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œè¯„åˆ†ï¼Œç§»é™¤æ‰€æœ‰ç¡¬ç¼–ç è§„åˆ™å’ŒçŸ¥è¯†åº“
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import re
import json
import hashlib
from functools import lru_cache
from typing import Dict, List, Any, Optional
from utils.logger_utils import get_logger
from llm_client import InternalAPIClient, AIBuildersAPIClient
from config.llm_config import get_batch_evaluation_params
from utils.prompt_manager import get_prompt_manager

logger = get_logger('result_scorer')


# ==============================================================================
# LLMè°ƒç”¨ç¼“å­˜ï¼ˆä½¿ç”¨ functools.lru_cacheï¼‰
# ==============================================================================
@lru_cache(maxsize=1000)
def _call_llm_with_cache(
    cache_key: str,
    system_prompt: str,
    user_prompt: str,
    max_tokens: int,
    temperature: float
) -> str:
    """
    å¸¦ç¼“å­˜çš„LLMè°ƒç”¨ï¼ˆä½¿ç”¨ functools.lru_cacheï¼‰

    Args:
        cache_key: ç¼“å­˜é”®ï¼ˆMD5å“ˆå¸Œï¼‰
        system_prompt: ç³»ç»Ÿæç¤º
        user_prompt: ç”¨æˆ·æç¤º
        max_tokens: æœ€å¤§tokenæ•°
        temperature: æ¸©åº¦å‚æ•°

    Returns:
        LLMå“åº”æ–‡æœ¬

    Note:
        æ­¤å‡½æ•°åœ¨æ¨¡å—çº§åˆ«å®šä¹‰ï¼Œä»¥ä¾¿ä½¿ç”¨ lru_cache
        å®é™…çš„LLMè°ƒç”¨é€šè¿‡å†…éƒ¨çš„ _llm_client_for_cache å®Œæˆ
    """
    # è·å–å…¨å±€LLMå®¢æˆ·ç«¯ï¼ˆéœ€è¦åœ¨ç±»åˆå§‹åŒ–æ—¶è®¾ç½®ï¼‰
    global _llm_client_for_cache
    if _llm_client_for_cache is None:
        logger.warning("LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºå“åº”")
        return "[]"

    try:
        response = _llm_client_for_cache.call_llm(
            prompt=user_prompt,
            system_prompt=system_prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )
        return response
    except Exception as e:
        logger.error(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
        return "[]"


# å…¨å±€LLMå®¢æˆ·ç«¯ï¼ˆç”¨äºç¼“å­˜å‡½æ•°ï¼‰
_llm_client_for_cache = None


class IntelligentResultScorer:
    """
    æ™ºèƒ½ç»“æœè¯„åˆ†å™¨ - çº¯LLMç‰ˆæœ¬
    
    åªä½¿ç”¨å¤§æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè¯„ä¼°ï¼Œç§»é™¤æ‰€æœ‰ç¡¬ç¼–ç è¯„åˆ†è§„åˆ™å’ŒçŸ¥è¯†åº“
    """

    def __init__(self, country_code: str = None, log_collector=None):
        """
        åˆå§‹åŒ–è¯„åˆ†å™¨

        Args:
            country_code: å›½å®¶ä»£ç  (ä¿ç•™å‚æ•°å…¼å®¹æ€§ï¼Œä½†ä¸ä½¿ç”¨)
            log_collector: æœç´¢æ—¥å¿—æ”¶é›†å™¨ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºè®°å½•æ¨¡å‹è°ƒç”¨
        """
        self.log_collector = log_collector  # ä¿å­˜æ—¥å¿—æ”¶é›†å™¨å¼•ç”¨
        self.prompt_mgr = get_prompt_manager()  # åˆå§‹åŒ–æç¤ºè¯ç®¡ç†å™¨
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ï¼ˆä¼˜å…ˆä½¿ç”¨ Internal API çš„ gemini-2.5-proï¼‰
        try:
            self.llm_client = InternalAPIClient(model_type='vision')  # ä½¿ç”¨ vision ç±»å‹ï¼Œå®é™…ä¼šç”¨ gemini-2.5-pro
            self.model_name = 'gemini-2.5-pro'
            logger.info(f"âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨å…¬å¸å†…éƒ¨ API (gemini-2.5-pro)")
        except Exception as e:
            logger.warning(f"âš ï¸ å…¬å¸å†…éƒ¨ API åˆå§‹åŒ–å¤±è´¥: {str(e)}ï¼Œå°è¯• AI Builders API")
            try:
                self.llm_client = AIBuildersAPIClient()
                self.model_name = 'deepseek'
                logger.info(f"âœ… ä½¿ç”¨ AI Builders APIï¼Œæ¨¡å‹: {self.model_name}")
            except Exception as e2:
                logger.error(f"âŒ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e2)}")
                self.llm_client = None
                self.model_name = 'none'

        # è®¾ç½®å…¨å±€LLMå®¢æˆ·ç«¯ï¼ˆç”¨äºç¼“å­˜å‡½æ•°ï¼‰
        global _llm_client_for_cache
        _llm_client_for_cache = self.llm_client

        logger.info("âœ… è¯„åˆ†å™¨åˆå§‹åŒ–å®Œæˆï¼ˆçº¯LLMæ¨¡å¼ï¼Œä½¿ç”¨ lru_cache)")

    # ==============================================================================
    # ç¼“å­˜é”®ç”Ÿæˆ
    # ==============================================================================
    def _generate_llm_cache_key(self, batch: List[Dict[str, Any]], query: str, metadata: Optional[Dict] = None) -> str:
        """ç”ŸæˆLLMç¼“å­˜é”®"""
        key_data = {
            'query': query,
            'metadata': metadata,
            'titles': [r.get('title', '') for r in batch]
        }
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()

    def _safe_extract_metadata(self, metadata: Optional[Dict], field: str, max_len: int = 50) -> str:
        """
        å®‰å…¨æå–å…ƒæ•°æ®å­—æ®µ

        Args:
            metadata: å…ƒæ•°æ®å­—å…¸
            field: å­—æ®µåï¼ˆä¼šè‡ªåŠ¨å°è¯• field_name ä½œä¸ºåå¤‡ï¼‰
            max_len: æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤50ï¼‰

        Returns:
            æå–çš„å­—ç¬¦ä¸²å€¼ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        if not metadata:
            return ''

        # å°è¯•è·å–å­—æ®µï¼ˆæ”¯æŒ field å’Œ field_name ä¸¤ç§å½¢å¼ï¼‰
        value = metadata.get(field) or metadata.get(f'{field}_name', '')

        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶é™åˆ¶é•¿åº¦
        return str(value)[:max_len] if value else ''

    # ==============================================================================
    # é»‘åå•è¿‡æ»¤ï¼ˆå®‰å…¨åŠŸèƒ½ï¼Œä¿ç•™ï¼‰
    # ==============================================================================
    def _should_filter_by_blacklist(self, result: Dict[str, Any], metadata: Optional[Dict] = None) -> tuple[bool, str]:
        """
        æ£€æŸ¥ç»“æœæ˜¯å¦åº”è¯¥è¢«é»‘åå•è¿‡æ»¤
        
        Returns:
            (should_filter, filter_reason)
        """
        # ç®€åŒ–çš„é»‘åå•æ£€æŸ¥
        blacklist_keywords = [
            'porn', 'xxx', 'casino', 'gambling', 'betting',
            'viagra', 'cialis', 'loan', 'debt', 'insurance'
        ]
        
        title = result.get('title', '').lower()
        url = result.get('url', '').lower()
        combined = f"{title} {url}"
        
        for keyword in blacklist_keywords:
            if keyword in combined:
                return True, f"åŒ…å«é»‘åå•å…³é”®è¯: {keyword}"
        
        return False, ""

    # ==============================================================================
    # MCPå·¥å…·ä¸°å¯Œï¼ˆä¿ç•™ç”¨äºè·å–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰
    # ==============================================================================
    def _enrich_result_with_mcp_tools(self, result: Dict[str, Any], metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨MCPå·¥å…·ä¸°å¯Œç»“æœä¿¡æ¯ï¼ˆè§†é¢‘ç¼©ç•¥å›¾ã€ç½‘é¡µå†…å®¹ç­‰ï¼‰
        è¿™äº›ä¿¡æ¯å°†ä½œä¸ºä¸Šä¸‹æ–‡ä¼ é€’ç»™LLMï¼Œè€Œä¸æ˜¯ç›´æ¥ç”Ÿæˆè¯„åˆ†
        """
        # ç®€åŒ–ç‰ˆæœ¬ï¼šä¿ç•™ç»“æ„ä½†å®é™…ä¸åšä¸°å¯Œ
        # å®é™…ä¸°å¯Œé€»è¾‘å¯ä»¥åç»­æ·»åŠ 
        return result

    # ==============================================================================
    # LLMæ‰¹é‡è¯„ä¼°æ–¹æ³•ï¼ˆæ ¸å¿ƒï¼‰
    # ==============================================================================
    def _evaluate_batch_with_llm(self, results: List[Dict[str, Any]], query: str, metadata: Optional[Dict] = None) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨LLMæ‰¹é‡è¯„ä¼°å¤šä¸ªç»“æœ
        
        Args:
            results: æœç´¢ç»“æœåˆ—è¡¨ï¼ˆæœ€å¤š10ä¸ªç»“æœï¼‰
            query: æœç´¢æŸ¥è¯¢
            metadata: é¢å¤–çš„å…ƒæ•°æ®
        
        Returns:
            åŒ…å«è¯„åˆ†çš„ç»“æœåˆ—è¡¨
        """
        if not self.llm_client or not results:
            return results
        
        # âœ¨ ä¼˜åŒ–æ€§èƒ½ï¼šå‡å°‘æ‰¹é‡å¤§å°ï¼ˆ10ä¸ª â†’ 5ä¸ªï¼‰ï¼Œé™ä½å•æ¬¡LLMè¯„åˆ†æ—¶é—´ï¼Œé¿å…è¶…æ—¶
        # é…åˆå‰ç«¯è¶…æ—¶ä»180ç§’å¢åŠ åˆ°300ç§’çš„ä¼˜åŒ–ï¼Œç¡®ä¿æœç´¢è¯·æ±‚åœ¨åˆç†æ—¶é—´å†…å®Œæˆ
        BATCH_SIZE = 5
        batches = [results[i:i + BATCH_SIZE] for i in range(0, len(results), BATCH_SIZE)]
        
        scored_results = []
        for batch in batches:
            batch_scores = self._call_llm_for_batch(batch, query, metadata)
            scored_results.extend(batch_scores)
        
        return scored_results

    def _call_llm_for_batch(self, batch: List[Dict[str, Any]], query: str, metadata: Optional[Dict] = None) -> List[Dict[str, Any]]:
        """ä¸ºä¸€ä¸ªæ‰¹æ¬¡çš„ç»“æœè°ƒç”¨LLMè¿›è¡Œæ‰¹é‡è¯„åˆ†"""
        if not self.llm_client:
            return batch
        
        try:
            # âœ¨ ä»æç¤ºè¯ç®¡ç†å™¨è·å–ç³»ç»Ÿæç¤ºè¯ï¼ˆæ›¿ä»£ç¡¬ç¼–ç ï¼‰
            system_prompt = self.prompt_mgr.get_batch_scoring_system_prompt()

            # è·å–å…ƒæ•°æ®ï¼ˆä½¿ç”¨è¾…åŠ©æ–¹æ³•ï¼‰
            safe_grade = self._safe_extract_metadata(metadata, 'grade')
            safe_subject = self._safe_extract_metadata(metadata, 'subject')
            safe_query = query[:200] if query else ''

            # âœ¨ ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨æ„å»ºç”¨æˆ·æç¤ºè¯ï¼ˆæ›¿ä»£ç¡¬ç¼–ç ï¼‰
            user_prompt = self.prompt_mgr.get_batch_scoring_user_prompt(
                grade=safe_grade,
                subject=safe_subject,
                query=safe_query,
                results=batch
            )

            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = self._generate_llm_cache_key(batch, query, metadata)

            # è®°å½•å¼€å§‹æ—¶é—´
            import time
            start_time = time.time()

            # è·å–LLMå‚æ•°ï¼ˆä½¿ç”¨é…ç½®ç®¡ç†ï¼‰
            llm_params = get_batch_evaluation_params(max_results=len(batch))

            # è°ƒç”¨LLMï¼ˆä½¿ç”¨ lru_cache è‡ªåŠ¨ç¼“å­˜ï¼‰
            response = _call_llm_with_cache(
                cache_key=cache_key,
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                max_tokens=llm_params['max_tokens'],
                temperature=llm_params['temperature']
            )

            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            execution_time = time.time() - start_time

            # âœ… è®°å½•LLMè°ƒç”¨åˆ°æ—¥å¿—æ”¶é›†å™¨
            if self.log_collector:
                try:
                    # æ„å»ºè¾“å…¥ä¿¡æ¯æ‘˜è¦ï¼ˆä½¿ç”¨è¾…åŠ©æ–¹æ³•ï¼‰
                    input_summary = f"æ‰¹é‡è¯„ä¼° {len(batch)} ä¸ªæœç´¢ç»“æœ\n"
                    input_summary += f"ç›®æ ‡å¹´çº§: {self._safe_extract_metadata(metadata, 'grade')}\n"
                    input_summary += f"ç›®æ ‡å­¦ç§‘: {self._safe_extract_metadata(metadata, 'subject')}"

                    # æˆªå–è¾“å‡ºç»“æœï¼ˆé™åˆ¶é•¿åº¦ï¼‰
                    output_summary = response[:500] + "..." if len(response) > 500 else response

                    # è®°å½•LLMè°ƒç”¨
                    # æ ¹æ®æ¨¡å‹åç§°ç¡®å®šæä¾›å•†
                    provider = "Internal API" if "gemini" in self.model_name else "AI Builders API"

                    self.log_collector.record_llm_call(
                        model_name=self.model_name,
                        function="æ‰¹é‡ç»“æœè¯„åˆ†",
                        provider=provider,
                        prompt=user_prompt[:200] + "..." if len(user_prompt) > 200 else user_prompt,
                        input_data=input_summary,
                        output_data=output_summary,
                        execution_time=execution_time,
                        tokens_used=None,  # æš‚æœªè¿”å›tokenæ•°
                        cost=None
                    )
                    logger.debug(f"        [ğŸ“ æ—¥å¿—] æ‰¹é‡è¯„åˆ†LLMè°ƒç”¨å·²è®°å½•")
                except Exception as log_err:
                    logger.warning(f"        [âš ï¸ è­¦å‘Š] è®°å½•æ‰¹é‡è¯„åˆ†LLMè°ƒç”¨å¤±è´¥: {log_err}")

            # è§£æå“åº”
            scored_batch = self._parse_batch_response(response, batch)

            return scored_batch

        except Exception as e:
            logger.warning(f"æ‰¹é‡LLMè¯„ä¼°å¤±è´¥: {str(e)[:200]}")
            # è¿”å›åŸå§‹ç»“æœï¼ˆåç»­ä¼šé‡è¯•ï¼‰
            return batch

    def _parse_batch_response(self, response: str, original_batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è§£ææ‰¹é‡LLMå“åº”å¹¶æ›´æ–°åŸå§‹ç»“æœ"""
        try:
            # å°è¯•æå–JSON
            from core.json_utils import extract_json_array
            scores_array = extract_json_array(response)

            if not scores_array or len(scores_array) == 0:
                raise ValueError("æœªèƒ½æå–æœ‰æ•ˆçš„JSONæ•°ç»„")

            scored_results = []

            for idx, item in enumerate(original_batch):
                result_copy = item.copy()
                original_index = idx
                matching_score = None

                for score_item in scores_array:
                    if score_item.get('index') == original_index:
                        matching_score = score_item
                        break

                if matching_score:
                    result_copy['score'] = matching_score.get('score', 0.0)
                    result_copy['recommendation_reason'] = matching_score.get('reason', 'LLMæ‰¹é‡è¯„ä¼°')
                    result_copy['evaluation_method'] = 'LLM (Batch)'
                else:
                    logger.warning(f"ç´¢å¼• {original_index} æœªæ‰¾åˆ°è¯„åˆ†")
                    result_copy['evaluation_method'] = 'LLM (Batch) - æœªæ‰¾åˆ°è¯„åˆ†'
                
                scored_results.append(result_copy)

            return scored_results

        except Exception as e:
            logger.error(f"è§£ææ‰¹é‡å“åº”å¤±è´¥: {str(e)[:200]}")
            return original_batch

    # ==============================================================================
    # å•ä¸ªç»“æœLLMè¯„ä¼°
    # ==============================================================================
    def _evaluate_with_llm(self, result: Dict[str, Any], query: str, metadata: Optional[Dict] = None) -> Optional[Dict[str, Any]]:
        """
        ä½¿ç”¨LLMè¯„ä¼°å•ä¸ªç»“æœ
        
        Returns:
            åŒ…å«scoreå’Œrecommendation_reasonçš„å­—å…¸
        """
        if not self.llm_client:
            return None

        try:
            title_debug = result.get('title', 'Unknown')[:50]
            logger.info(f"[ğŸ” LLMè¯„ä¼°] å¼€å§‹è¯„ä¼°: {title_debug}...")

            # æ„å»ºè¯„ä¼°æç¤ºè¯
            title = result.get('title', '')
            url = result.get('url', '')
            snippet = result.get('snippet', '')

            # è·å–å…ƒæ•°æ®ï¼ˆä½¿ç”¨è¾…åŠ©æ–¹æ³•ï¼‰
            safe_grade = self._safe_extract_metadata(metadata, 'grade')
            safe_subject = self._safe_extract_metadata(metadata, 'subject')

            # âœ¨ ä»æç¤ºè¯ç®¡ç†å™¨è·å–ç³»ç»Ÿæç¤ºè¯ï¼ˆæ›¿ä»£ç¡¬ç¼–ç ï¼‰
            system_prompt = self.prompt_mgr.get_single_scoring_system_prompt()

            # âœ¨ ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨æ„å»ºç”¨æˆ·æç¤ºè¯ï¼ˆæ›¿ä»£ç¡¬ç¼–ç ï¼‰
            user_prompt = self.prompt_mgr.get_single_scoring_user_prompt(
                grade=safe_grade,
                subject=safe_subject,
                query=query,
                result=result
            )

            # è®°å½•å¼€å§‹æ—¶é—´
            import time
            start_time = time.time()

            # è·å–LLMå‚æ•°ï¼ˆä½¿ç”¨é…ç½®ç®¡ç†ï¼‰
            llm_params = get_batch_evaluation_params(max_results=1)

            # è°ƒç”¨LLM
            response = self.llm_client.call_llm(
                prompt=user_prompt,
                system_prompt=system_prompt,
                max_tokens=llm_params['max_tokens'],
                temperature=llm_params['temperature']
            )

            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            execution_time = time.time() - start_time

            # âœ… è®°å½•LLMè°ƒç”¨åˆ°æ—¥å¿—æ”¶é›†å™¨
            if self.log_collector:
                try:
                    # æ„å»ºè¾“å…¥ä¿¡æ¯æ‘˜è¦
                    input_summary = f"è¯„ä¼°å•ä¸ªæœç´¢ç»“æœ\n"
                    input_summary += f"æ ‡é¢˜: {result.get('title', '')[:100]}\n"
                    input_summary += f"ç›®æ ‡å¹´çº§: {safe_grade}\n"
                    input_summary += f"ç›®æ ‡å­¦ç§‘: {safe_subject}"

                    # æˆªå–è¾“å‡ºç»“æœï¼ˆé™åˆ¶é•¿åº¦ï¼‰
                    output_summary = response[:300] + "..." if len(response) > 300 else response

                    # è®°å½•LLMè°ƒç”¨
                    # æ ¹æ®æ¨¡å‹åç§°ç¡®å®šæä¾›å•†
                    provider = "Internal API" if "gemini" in self.model_name else "AI Builders API"

                    self.log_collector.record_llm_call(
                        model_name=self.model_name,
                        function="å•ä¸ªç»“æœè¯„åˆ†",
                        provider=provider,
                        prompt=user_prompt[:200] + "..." if len(user_prompt) > 200 else user_prompt,
                        input_data=input_summary,
                        output_data=output_summary,
                        execution_time=execution_time,
                        tokens_used=None,
                        cost=None
                    )
                    logger.debug(f"        [ğŸ“ æ—¥å¿—] å•ä¸ªè¯„åˆ†LLMè°ƒç”¨å·²è®°å½•")
                except Exception as log_err:
                    logger.warning(f"        [âš ï¸ è­¦å‘Š] è®°å½•å•ä¸ªè¯„åˆ†LLMè°ƒç”¨å¤±è´¥: {log_err}")

            # è§£æå“åº”
            import json
            try:
                # ğŸ”§ å…ˆæ¸…ç†å“åº”ä¸­çš„markdownä»£ç å—æ ‡è®°
                cleaned_response = response.strip()
                if cleaned_response.startswith('```'):
                    # ç§»é™¤markdownä»£ç å—æ ‡è®°
                    lines = cleaned_response.split('\n')
                    if lines[0].startswith('```'):
                        lines = lines[1:]  # ç§»é™¤ç¬¬ä¸€è¡Œæ ‡è®°
                    if lines and lines[-1].startswith('```'):
                        lines = lines[:-1]  # ç§»é™¤æœ€åä¸€è¡Œæ ‡è®°
                    cleaned_response = '\n'.join(lines)

                # å°è¯•è§£æJSON
                result_data = json.loads(cleaned_response)
                score = float(result_data.get('score', 5.0))
                reason = result_data.get('reason', 'æ ¹æ®æœç´¢åŒ¹é…åº¦æ¨è')
            except:
                # å°è¯•æ­£åˆ™æå–ï¼ˆå¤„ç†è¢«æˆªæ–­çš„å“åº”ï¼‰
                score_match = re.search(r'"score"\s*:\s*([\d.]+)', response)
                # æ”¹è¿›çš„æ­£åˆ™ï¼šåŒ¹é…reasonå­—æ®µï¼Œå³ä½¿åŒ…å«æ¢è¡Œæˆ–æœªé—­åˆ
                reason_match = re.search(r'"reason"\s*:\s*"([^"]*(?:"[^"]*)*)', response, re.DOTALL)

                if score_match:
                    score = float(score_match.group(1))
                    if reason_match:
                        reason = reason_match.group(1)
                        # æ¸…ç†reasonä¸­çš„è½¬ä¹‰å­—ç¬¦
                        reason = reason.replace('\\"', '"').replace('\\n', '\n')
                        if len(reason) > 100:
                            reason = reason[:100]
                    else:
                        reason = "LLMè¯„ä¼°å“åº”è¢«æˆªæ–­"
                    logger.info(f"âœ… LLMè¯„ä¼°æˆåŠŸï¼ˆæ­£åˆ™è§£æï¼‰: score={score:.1f}")
                else:
                    logger.error(f"æ— æ³•è§£æLLMå“åº”: {response[:200]}")
                    return None

            # ç¡®ä¿åˆ†æ•°åœ¨0-10èŒƒå›´å†…
            score = max(0.0, min(10.0, score))

            logger.info(f"âœ… LLMè¯„ä¼°æˆåŠŸ: score={score:.1f}, reason={reason[:30]}...")

            return {
                'score': score,
                'recommendation_reason': reason,
                'evaluation_method': 'LLM'
            }

        except Exception as e:
            logger.warning(f"âš ï¸ LLMè¯„ä¼°å¤±è´¥: {str(e)[:100]}")
            return None

    # ==============================================================================
    # ä¸»è¯„ä¼°å…¥å£
    # ==============================================================================
    def score_results(self, results: List[Dict[str, Any]], query: str, metadata: Optional[Dict] = None) -> List[Dict[str, Any]]:
        """
        å¯¹å¤šä¸ªç»“æœè¿›è¡Œè¯„åˆ†ï¼ˆçº¯LLMç‰ˆæœ¬ï¼‰
        
        Args:
            results: æœç´¢ç»“æœåˆ—è¡¨
            query: æœç´¢æŸ¥è¯¢
            metadata: é¢å¤–çš„å…ƒæ•°æ®
        
        Returns:
            è¯„åˆ†åçš„ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸ“Š å¼€å§‹æ‰¹é‡è¯„ä¼° {len(results)} ä¸ªæœç´¢ç»“æœï¼ˆçº¯LLMæ¨¡å¼ï¼‰")

        # æ­¥éª¤0: é»‘åå•å‰ç½®è¿‡æ»¤
        filtered_results = []
        filtered_count = 0

        for result in results:
            should_filter, filter_reason = self._should_filter_by_blacklist(result, metadata)

            if should_filter:
                filtered_count += 1
                logger.warning(f"[ğŸš« é»‘åå•è¿‡æ»¤] {filter_reason}: {result.get('title', '')[:50]}")
                result['score'] = 0.0
                result['filtered'] = True
                result['filter_reason'] = filter_reason
                result['recommendation_reason'] = f"è§¦å‘é»‘åå•è¿‡æ»¤ï¼š{filter_reason}"
                result['evaluation_method'] = 'Blacklist'
                filtered_results.append(result)
            else:
                result['filtered'] = False
                filtered_results.append(result)

        logger.info(f"[ğŸ“Š é»‘åå•è¿‡æ»¤ç»Ÿè®¡] æ€»è®¡: {len(results)}, è¿‡æ»¤: {filtered_count}, ä¿ç•™: {len(results) - filtered_count}")

        # æ­¥éª¤1: ä½¿ç”¨æ‰¹é‡LLMè¯„ä¼°
        try:
            scored_results = self._evaluate_batch_with_llm(filtered_results, query, metadata)
            batch_llm_count = sum(1 for r in scored_results if r.get('evaluation_method') == 'LLM (Batch)')
            logger.info(f"âœ… æ‰¹é‡LLMè¯„ä¼°å®Œæˆ: {len(filtered_results)}ä¸ªç»“æœï¼Œ{batch_llm_count}ä¸ªä½¿ç”¨æ‰¹é‡LLMè¯„ä¼°")
            
            if batch_llm_count > 0:
                return scored_results
        except Exception as e:
            logger.warning(f"æ‰¹é‡LLMè¯„ä¼°å¤±è´¥: {str(e)[:200]}")

        # æ­¥éª¤2: é™çº§åˆ°é€ä¸ªLLMè¯„ä¼°
        logger.info(f"ğŸ“Š é™çº§åˆ°é€ä¸ªLLMè¯„ä¼°")
        scored_results = []
        
        for idx, result in enumerate(filtered_results):
            try:
                llm_evaluation = self._evaluate_with_llm(result, query, metadata)
                if llm_evaluation:
                    result['score'] = llm_evaluation['score']
                    result['recommendation_reason'] = llm_evaluation['recommendation_reason']
                    result['evaluation_method'] = llm_evaluation.get('evaluation_method', 'LLM')
                else:
                    # LLMè¯„ä¼°å¤±è´¥ï¼Œè®¾ç½®é»˜è®¤å€¼
                    result['score'] = 5.0
                    result['recommendation_reason'] = 'LLMè¯„ä¼°å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥'
                    result['evaluation_method'] = 'Failed'
                
                scored_results.append(result)
            except Exception as e:
                logger.error(f"ç»“æœè¯„ä¼°å¤±è´¥ (ç´¢å¼•{idx}): {str(e)[:100]}")
                result['score'] = 5.0
                result['recommendation_reason'] = 'è¯„ä¼°å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥'
                result['evaluation_method'] = 'Error'
                scored_results.append(result)

        llm_count = sum(1 for r in scored_results if r.get('evaluation_method') in ['LLM', 'LLM (Batch)'])
        logger.info(f"âœ… è¯„ä¼°å®Œæˆ: {len(results)}ä¸ªç»“æœ (LLM: {llm_count})")

        return scored_results


# ==============================================================================
# å…¨å±€è¾…åŠ©å‡½æ•°ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
# ==============================================================================
_result_scorer_instance = None

def get_result_scorer(country_code: str = None, log_collector=None) -> IntelligentResultScorer:
    """
    è·å–è¯„åˆ†å™¨å•ä¾‹å®ä¾‹

    Args:
        country_code: å›½å®¶ä»£ç 
        log_collector: æœç´¢æ—¥å¿—æ”¶é›†å™¨ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºè®°å½•æ¨¡å‹è°ƒç”¨

    Returns:
        IntelligentResultScorerå®ä¾‹
    """
    # å¦‚æœä¼ å…¥äº†log_collectorï¼Œåˆ›å»ºæ–°å®ä¾‹ï¼ˆä¸ä½¿ç”¨å…¨å±€å•ä¾‹ï¼‰
    if log_collector is not None:
        return IntelligentResultScorer(country_code, log_collector)

    # ä½¿ç”¨å…¨å±€å•ä¾‹ï¼ˆå‘åå…¼å®¹ï¼‰
    global _result_scorer_instance
    if _result_scorer_instance is None:
        _result_scorer_instance = IntelligentResultScorer(country_code)
    return _result_scorer_instance
