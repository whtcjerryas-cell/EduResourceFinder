#!/usr/bin/env python3
"""
A/Bæµ‹è¯•ä¸»å…¥å£

è¿è¡ŒLLMæ¨¡å‹A/Bæµ‹è¯•ï¼Œæ”¯æŒï¼š
- æ™ºèƒ½è¯„åˆ†æµ‹è¯•
- æœç´¢ç­–ç•¥æµ‹è¯•ï¼ˆTODOï¼‰
- æ¨èç†ç”±æµ‹è¯•ï¼ˆTODOï¼‰
"""
import sys
import argparse
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from utils.logger_utils import get_logger
from tests.ab_testing.runners.scoring_test_runner import ScoringTestRunner

logger = get_logger('run_ab_test')


def main():
    parser = argparse.ArgumentParser(
        description="è¿è¡ŒLLMæ¨¡å‹A/Bæµ‹è¯•",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è¿è¡Œæ™ºèƒ½è¯„åˆ†æµ‹è¯•ï¼ˆæ‰€æœ‰æ¨¡å‹ï¼‰
  python tests/ab_testing/run_ab_test.py --test-type scoring

  # è¿è¡Œæ™ºèƒ½è¯„åˆ†æµ‹è¯•ï¼ˆæŒ‡å®šæ¨¡å‹ï¼‰
  python tests/ab_testing/run_ab_test.py --test-type scoring --models gemini-2.5-pro gemini-2.5-flash

  # è¿è¡Œæ™ºèƒ½è¯„åˆ†æµ‹è¯•ï¼ˆé™åˆ¶æµ‹è¯•ç”¨ä¾‹æ•°ï¼‰
  python tests/ab_testing/run_ab_test.py --test-type scoring --test-cases 10

  # è¯¦ç»†è¾“å‡º
  python tests/ab_testing/run_ab_test.py --test-type scoring --verbose
        """
    )

    parser.add_argument(
        "--test-type",
        choices=["scoring", "strategy", "recommendation", "all"],
        default="scoring",
        help="æµ‹è¯•ç±»å‹ï¼ˆé»˜è®¤: scoringï¼‰"
    )
    parser.add_argument(
        "--models",
        nargs="+",
        help="è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨ï¼ˆé»˜è®¤æµ‹è¯•æ‰€æœ‰é…ç½®çš„æ¨¡å‹ï¼‰"
    )
    parser.add_argument(
        "--test-cases",
        type=int,
        help="æµ‹è¯•ç”¨ä¾‹æ•°é‡é™åˆ¶"
    )
    parser.add_argument(
        "--output-dir",
        default=str(Path(__file__).parent.parent / "reports" / "weekly_reports"),
        help="æŠ¥å‘Šè¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="è¯¦ç»†è¾“å‡º"
    )

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # æµ‹è¯•é…ç½®
    test_configs = {
        "scoring": {
            "name": "æ™ºèƒ½è¯„åˆ†æµ‹è¯•",
            "models": args.models or [
                "gemini-3-pro-thinking-high",
                "gemini-3-pro-thinking-medium",
                "gpt-5.2-thinking-medium",
                "claude-3-7-sonnet",
                "gemini-2.5-pro",
                "gemini-2.5-flash",
            ],
            "test_cases_file": "test_cases_scoring.json",
            "runner": ScoringTestRunner,
        },
        "strategy": {
            "name": "æœç´¢ç­–ç•¥æµ‹è¯•",
            "models": args.models or [
                "gemini-2.5-pro",
                "gemini-2.5-flash",
                "claude-3-7-sonnet",
                "gpt-4o",
                "grok-4-fast",
            ],
            "test_cases_file": "test_cases_strategy.json",
            "runner": None,  # TODO: å®ç°
        },
        "recommendation": {
            "name": "æ¨èç†ç”±æµ‹è¯•",
            "models": args.models or [
                "gemini-2.5-pro",
                "claude-3-7-sonnet",
                "gpt-4o",
                "gemini-2.5-flash",
            ],
            "test_cases_file": "test_cases_recommendation.json",
            "runner": None,  # TODO: å®ç°
        },
    }

    # è¿è¡Œæµ‹è¯•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    if args.test_type in ["scoring", "all"]:
        print("\n" + "="*80)
        print("ğŸ§ª å¼€å§‹æ™ºèƒ½è¯„åˆ†æµ‹è¯•")
        print("="*80)

        config = test_configs["scoring"]

        if config["runner"] is None:
            print(f"âš ï¸ {config['name']}å°šæœªå®ç°ï¼Œè·³è¿‡")
        else:
            print(f"\nğŸ“‹ æµ‹è¯•é…ç½®:")
            print(f"  - æ¨¡å‹æ•°é‡: {len(config['models'])}")
            print(f"  - æ¨¡å‹åˆ—è¡¨: {', '.join(config['models'])}")
            print(f"  - æµ‹è¯•ç”¨ä¾‹æ•°: {args.test_cases or 'å…¨éƒ¨'}")
            print(f"  - è¯¦ç»†è¾“å‡º: {'æ˜¯' if args.verbose else 'å¦'}")

            runner = config["runner"](
                models=config["models"],
                test_cases_limit=args.test_cases,
                verbose=args.verbose
            )

            results = runner.run()

            # ä¿å­˜ç»“æœ
            results_path = output_dir / f"scoring_test_{timestamp}.json"
            runner.save_results(results_path)

            # ç”ŸæˆæŠ¥å‘Š
            report_path = output_dir / f"scoring_report_{timestamp}.md"
            runner.generate_report(report_path)

            print(f"\nâœ… {config['name']}å®Œæˆï¼")
            print(f"  - ç»“æœ: {results_path}")
            print(f"  - æŠ¥å‘Š: {report_path}")

    if args.test_type in ["strategy", "all"]:
        print("\n" + "="*80)
        print("ğŸ§ª å¼€å§‹æœç´¢ç­–ç•¥æµ‹è¯•")
        print("="*80)

        config = test_configs["strategy"]

        if config["runner"] is None:
            print(f"âš ï¸ {config['name']}å°šæœªå®ç°ï¼Œè·³è¿‡")
        else:
            # TODO: å®ç°æœç´¢ç­–ç•¥æµ‹è¯•
            pass

    if args.test_type in ["recommendation", "all"]:
        print("\n" + "="*80)
        print("ğŸ§ª å¼€å§‹æ¨èç†ç”±æµ‹è¯•")
        print("="*80)

        config = test_configs["recommendation"]

        if config["runner"] is None:
            print(f"âš ï¸ {config['name']}å°šæœªå®ç°ï¼Œè·³è¿‡")
        else:
            # TODO: å®ç°æ¨èç†ç”±æµ‹è¯•
            pass

    print("\n" + "="*80)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
    print("="*80)


if __name__ == "__main__":
    main()
