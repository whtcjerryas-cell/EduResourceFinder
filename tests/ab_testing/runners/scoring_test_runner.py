#!/usr/bin/env python3
"""
æ™ºèƒ½è¯„åˆ†æµ‹è¯•æ‰§è¡Œå™¨

æµ‹è¯•ä¸åŒæ¨¡å‹åœ¨æ™ºèƒ½è¯„åˆ†ä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§å’Œæ€§èƒ½
"""
import sys
import json
import time
import os
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from logger_utils import get_logger
from tests.ab_testing.utils.llm_caller import LLMCaller

logger = get_logger('scoring_test_runner')


class ScoringTestRunner:
    """æ™ºèƒ½è¯„åˆ†æµ‹è¯•æ‰§è¡Œå™¨"""

    def __init__(
        self,
        models: List[str],
        test_cases_limit: int = None,
        verbose: bool = False
    ):
        """
        åˆå§‹åŒ–æµ‹è¯•æ‰§è¡Œå™¨

        Args:
            models: è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨
            test_cases_limit: æµ‹è¯•ç”¨ä¾‹æ•°é‡é™åˆ¶
            verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        """
        self.models = models
        self.test_cases_limit = test_cases_limit
        self.verbose = verbose
        self.results = []

        # åŠ è½½æµ‹è¯•ç”¨ä¾‹
        self._load_test_cases()

        # åˆå§‹åŒ–LLMè°ƒç”¨å™¨
        self.llm_caller = LLMCaller()

    def _load_test_cases(self):
        """åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        test_cases_path = Path(__file__).parent.parent / "test_data" / "test_cases_scoring.json"

        if not test_cases_path.exists():
            raise FileNotFoundError(f"æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶ä¸å­˜åœ¨: {test_cases_path}")

        with open(test_cases_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.test_cases = data["test_cases"]

        # é™åˆ¶æµ‹è¯•ç”¨ä¾‹æ•°é‡
        if self.test_cases_limit:
            self.test_cases = self.test_cases[:self.test_cases_limit]

        logger.info(f"âœ… å·²åŠ è½½ {len(self.test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")

    def run(self) -> List[Dict[str, Any]]:
        """
        è¿è¡Œæµ‹è¯•

        Returns:
            æµ‹è¯•ç»“æœåˆ—è¡¨
        """
        total_tests = len(self.models) * len(self.test_cases)
        current_test = 0

        for model in self.models:
            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸ§ª æµ‹è¯•æ¨¡å‹: {model}")
            logger.info(f"{'='*80}")

            model_results = {
                "model": model,
                "test_results": [],
                "statistics": {
                    "total_tests": len(self.test_cases),
                    "total_time": 0,
                    "average_time": 0,
                }
            }

            for test_case in self.test_cases:
                current_test += 1
                logger.info(f"\n[{current_test}/{total_tests}] æµ‹è¯•ç”¨ä¾‹: {test_case['id']}")

                # è¿è¡Œå•ä¸ªæµ‹è¯•
                test_result = self._run_single_test(model, test_case)
                model_results["test_results"].append(test_result)

                # æ‰“å°ç»“æœ
                if self.verbose:
                    self._print_test_result(test_result)

            # è®¡ç®—æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
            model_results["statistics"]["total_time"] = sum(
                r["execution_time"] for r in model_results["test_results"]
            )
            model_results["statistics"]["average_time"] = (
                model_results["statistics"]["total_time"] / len(model_results["test_results"])
            )

            self.results.append(model_results)

        # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
        self._calculate_summary_statistics()

        return self.results

    def _run_single_test(self, model: str, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¿è¡Œå•ä¸ªæµ‹è¯•

        Args:
            model: æ¨¡å‹åç§°
            test_case: æµ‹è¯•ç”¨ä¾‹

        Returns:
            æµ‹è¯•ç»“æœ
        """
        start_time = time.time()

        # å‡†å¤‡æµ‹è¯•æ•°æ®
        target = test_case["target"]
        search_results = test_case["search_results"]

        test_result = {
            "test_case_id": test_case["id"],
            "target": target,
            "model": model,
            "results": [],
            "execution_time": 0,
        }

        # å¯¹æ¯ä¸ªæœç´¢ç»“æœè¿›è¡Œè¯„åˆ†
        for search_result in search_results:
            logger.info(f"  - è¯„åˆ†: {search_result['title'][:50]}...")

            try:
                # è°ƒç”¨è¯„åˆ†å™¨ï¼ˆä½¿ç”¨æŒ‡å®šæ¨¡å‹ï¼‰
                score, reason, identified_info = self._score_with_model(
                    result=search_result,
                    query=target,
                    model=model
                )

                # è¯„ä¼°ç»“æœ
                expected = search_result.get("expected", {})
                evaluation = self._evaluate_score(
                    score=score,
                    reason=reason,
                    identified_info=identified_info,
                    expected=expected
                )

                result_eval = {
                    "title": search_result["title"],
                    "score": score,
                    "reason": reason,
                    "identified_info": identified_info,
                    "expected_score": expected.get("score"),
                    "evaluation": evaluation,
                    "success": evaluation["score_match"],
                }

            except Exception as e:
                logger.error(f"  âŒ è¯„åˆ†å¤±è´¥: {str(e)}")
                expected = search_result.get("expected", {})
                result_eval = {
                    "title": search_result["title"],
                    "score": None,
                    "reason": None,
                    "identified_info": None,
                    "expected_score": expected.get("score"),
                    "error": str(e),
                    "success": False,
                }

            test_result["results"].append(result_eval)

        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        test_result["execution_time"] = time.time() - start_time

        return test_result

    def _score_with_model(
        self,
        result: Dict[str, Any],
        query: Dict[str, Any],
        model: str
    ) -> tuple[float, str, Dict[str, Any]]:
        """
        ä½¿ç”¨æŒ‡å®šæ¨¡å‹è¿›è¡Œè¯„åˆ†

        Args:
            result: æœç´¢ç»“æœ
            query: æŸ¥è¯¢ä¿¡æ¯ï¼ˆå›½å®¶ã€å¹´çº§ã€å­¦ç§‘ï¼‰
            model: æ¨¡å‹åç§°

        Returns:
            (è¯„åˆ†, æ¨èç†ç”±, è¯†åˆ«ä¿¡æ¯)
        """
        # æ„å»ºè¯„åˆ†æç¤ºè¯
        target_grade = query["grade"]
        target_subject = query["subject"]
        country_code = query["country_code"]

        # è·å–å¹´çº§å’Œå­¦ç§‘çš„æ‰€æœ‰å˜ä½“
        grade_variants = query.get("grade_variants", [target_grade])
        subject_variants = query.get("subject_variants", [target_subject])

        # æ„å»ºæç¤ºè¯
        grade_variants_str = ", ".join(grade_variants[:3])
        subject_variants_str = ", ".join(subject_variants[:3])

        prompt = f"""è¯·ä¸ºä»¥ä¸‹æœç´¢ç»“æœè¯„åˆ†ï¼ˆ0-10åˆ†ï¼‰ï¼š

**æœç´¢ç›®æ ‡**: {country_code} {target_grade} {target_subject}

**ç›®æ ‡å¹´çº§è¡¨è¾¾**: {grade_variants_str}
**ç›®æ ‡å­¦ç§‘è¡¨è¾¾**: {subject_variants_str}

**æœç´¢ç»“æœ**:
æ ‡é¢˜: {result['title']}
æè¿°: {result.get('snippet', '')}

**è¯„åˆ†è¦æ±‚**:
1. å¹´çº§åŒ¹é…åº¦ï¼ˆ0-3åˆ†ï¼‰ï¼šä»æ ‡é¢˜ä¸­æå–å¹´çº§ï¼Œä¸ç›®æ ‡å¹´çº§å¯¹æ¯”
2. å­¦ç§‘åŒ¹é…åº¦ï¼ˆ0-3åˆ†ï¼‰ï¼šä»æ ‡é¢˜ä¸­æå–å­¦ç§‘ï¼Œä¸ç›®æ ‡å­¦ç§‘å¯¹æ¯”
3. èµ„æºè´¨é‡ï¼ˆ0-2åˆ†ï¼‰ï¼šåˆ¤æ–­æ˜¯å¦æ˜¯å®Œæ•´è¯¾ç¨‹/æ’­æ”¾åˆ—è¡¨
4. æ¥æºæƒå¨æ€§ï¼ˆ0-2åˆ†ï¼‰ï¼šåˆ¤æ–­æ¥æºæ˜¯å¦å¯ä¿¡

**è¯„åˆ†è§„åˆ™**:
- å¹´çº§ä¸ç¬¦å¿…é¡»å¤§å¹…å‡åˆ†ï¼ˆâ‰¤5åˆ†ï¼‰
- å­¦ç§‘ä¸ç¬¦å¿…é¡»å¤§å¹…å‡åˆ†ï¼ˆâ‰¤5åˆ†ï¼‰
- å®Œå…¨åŒ¹é…ç»™é«˜åˆ†ï¼ˆâ‰¥9åˆ†ï¼‰

**è¾“å‡ºæ ¼å¼**ï¼ˆJSONï¼‰:
{{
    "score": è¯„åˆ†ï¼ˆ0-10åˆ†ï¼Œæµ®ç‚¹æ•°ï¼‰,
    "identified_grade": "ä»æ ‡é¢˜ä¸­è¯†åˆ«çš„å¹´çº§",
    "identified_subject": "ä»æ ‡é¢˜ä¸­è¯†åˆ«çš„å­¦ç§‘",
    "reason": "è¯„åˆ†ç†ç”±ï¼ˆ30-50å­—ï¼‰"
}}

è¯·ç¡®ä¿è¾“å‡ºæ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚"""

        # è°ƒç”¨LLM
        llm_result = self.llm_caller.call_llm(
            prompt=prompt,
            model=model,
            max_tokens=200,
            temperature=0.3
        )

        if not llm_result["success"]:
            raise Exception(llm_result["error"])

        response = llm_result["response"]

        # è§£æå“åº”
        import re
        json_match = re.search(r'\{[^{}]*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            data = json.loads(json_str)
            score = float(data.get("score", 5.0))
            identified_grade = data.get("identified_grade", "")
            identified_subject = data.get("identified_subject", "")
            reason = data.get("reason", "")
        else:
            # è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
            logger.warning(f"âš ï¸ æ— æ³•è§£æJSONå“åº”ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†")
            score = 5.0
            identified_grade = ""
            identified_subject = ""
            reason = "è§£æå¤±è´¥"

        identified_info = {
            "grade": identified_grade,
            "subject": identified_subject,
        }

        return score, reason, identified_info

    def _evaluate_score(
        self,
        score: float,
        reason: str,
        identified_info: Dict[str, Any],
        expected: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°è¯„åˆ†ç»“æœ

        Args:
            score: å®é™…è¯„åˆ†
            reason: è¯„åˆ†ç†ç”±
            identified_info: è¯†åˆ«çš„å¹´çº§å’Œå­¦ç§‘
            expected: æœŸæœ›ç»“æœ

        Returns:
            è¯„ä¼°ç»“æœ
        """
        expected_score = expected.get("score", 5.0)
        score_range = expected.get("score_range", [expected_score - 1, expected_score + 1])

        # æ£€æŸ¥è¯„åˆ†æ˜¯å¦åœ¨é¢„æœŸèŒƒå›´å†…
        score_match = score_range[0] <= score <= score_range[1]

        # æ£€æŸ¥å¹´çº§è¯†åˆ«ï¼ˆå¦‚æœæœ‰æœŸæœ›å€¼ï¼‰
        grade_match = expected.get("grade_match", True)
        if identified_info["grade"]:
            expected_grade = expected.get("identified_grade", "")
            if expected_grade:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æœŸæœ›çš„å¹´çº§è¡¨è¾¾
                grade_match = expected_grade in identified_info["grade"] or identified_info["grade"] in expected_grade

        # æ£€æŸ¥å­¦ç§‘è¯†åˆ«ï¼ˆå¦‚æœæœ‰æœŸæœ›å€¼ï¼‰
        subject_match = expected.get("subject_match", True)
        if identified_info["subject"]:
            expected_subject = expected.get("identified_subject", "")
            if expected_subject:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æœŸæœ›çš„å­¦ç§‘è¡¨è¾¾
                subject_match = expected_subject in identified_info["subject"] or identified_info["subject"] in expected_subject

        return {
            "score_match": score_match,
            "grade_match": grade_match,
            "subject_match": subject_match,
            "score_deviation": abs(score - expected_score),
            "all_match": score_match and grade_match and subject_match,
        }

    def _print_test_result(self, test_result: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        logger.info(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {test_result['test_case_id']}")
        logger.info(f"  æ¨¡å‹: {test_result['model']}")
        logger.info(f"  æ‰§è¡Œæ—¶é—´: {test_result['execution_time']:.2f}ç§’")

        for i, result in enumerate(test_result["results"], 1):
            logger.info(f"\n  ç»“æœ {i}:")
            logger.info(f"    æ ‡é¢˜: {result['title'][:60]}...")
            logger.info(f"    å®é™…è¯„åˆ†: {result.get('score', 'N/A')}")
            logger.info(f"    æœŸæœ›è¯„åˆ†: {result.get('expected_score', 'N/A')}")
            logger.info(f"    åŒ¹é…: {'âœ…' if result.get('success') else 'âŒ'}")

            if "evaluation" in result:
                eval_ = result["evaluation"]
                logger.info(f"    è¯„åˆ†åŒ¹é…: {'âœ…' if eval_.get('score_match') else 'âŒ'}")
                logger.info(f"    å¹´çº§åŒ¹é…: {'âœ…' if eval_.get('grade_match') else 'âŒ'}")
                logger.info(f"    å­¦ç§‘åŒ¹é…: {'âœ…' if eval_.get('subject_match') else 'âŒ'}")
                logger.info(f"    è¯„åˆ†åå·®: {eval_.get('score_deviation', 0):.2f}")

            if result.get("identified_info"):
                logger.info(f"    è¯†åˆ«å¹´çº§: {result['identified_info'].get('grade', 'N/A')}")
                logger.info(f"    è¯†åˆ«å­¦ç§‘: {result['identified_info'].get('subject', 'N/A')}")

    def _calculate_summary_statistics(self):
        """è®¡ç®—æ±‡æ€»ç»Ÿè®¡"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š æ±‡æ€»ç»Ÿè®¡")
        logger.info("="*80)

        for model_result in self.results:
            model = model_result["model"]
            stats = model_result["statistics"]

            # è®¡ç®—å‡†ç¡®ç‡
            total_results = sum(len(r["results"]) for r in model_result["test_results"])
            successful_results = sum(
                sum(1 for r in test_result["results"] if r.get("success", False))
                for test_result in model_result["test_results"]
            )

            accuracy = successful_results / total_results if total_results > 0 else 0

            # è®¡ç®—å¹³å‡è¯„åˆ†åå·®
            score_deviations = [
                r["evaluation"]["score_deviation"]
                for test_result in model_result["test_results"]
                for r in test_result["results"]
                if "evaluation" in r and "score_deviation" in r["evaluation"]
            ]
            avg_score_deviation = sum(score_deviations) / len(score_deviations) if score_deviations else 0

            logger.info(f"\næ¨¡å‹: {model}")
            logger.info(f"  æ€»æµ‹è¯•æ•°: {stats['total_tests']}")
            logger.info(f"  æ€»ç»“æœæ•°: {total_results}")
            logger.info(f"  æˆåŠŸæ•°: {successful_results}")
            logger.info(f"  å‡†ç¡®ç‡: {accuracy:.2%}")
            logger.info(f"  å¹³å‡è¯„åˆ†åå·®: {avg_score_deviation:.2f}")
            logger.info(f"  æ€»è€—æ—¶: {stats['total_time']:.2f}ç§’")
            logger.info(f"  å¹³å‡è€—æ—¶: {stats['average_time']:.2f}ç§’")

    def save_results(self, output_path: Path):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)

        logger.info(f"\nâœ… æµ‹è¯•ç»“æœå·²ä¿å­˜: {output_path}")

    def generate_report(self, output_path: Path):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        from datetime import datetime

        lines = []
        lines.append("# æ™ºèƒ½è¯„åˆ†A/Bæµ‹è¯•æŠ¥å‘Š\n")
        lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        lines.append("---\n\n")

        # æ±‡æ€»ç»Ÿè®¡
        lines.append("## ğŸ“Š æ±‡æ€»ç»Ÿè®¡\n\n")
        lines.append("| æ¨¡å‹ | æ€»æµ‹è¯•æ•° | æˆåŠŸç‡ | å¹³å‡è¯„åˆ†åå·® | å¹³å‡è€—æ—¶ |\n")
        lines.append("|------|---------|--------|-------------|---------|\n")

        for model_result in self.results:
            model = model_result["model"]
            stats = model_result["statistics"]

            total_results = sum(len(r["results"]) for r in model_result["test_results"])
            successful_results = sum(
                sum(1 for r in test_result["results"] if r.get("success", False))
                for test_result in model_result["test_results"]
            )

            accuracy = successful_results / total_results if total_results > 0 else 0

            score_deviations = [
                r["evaluation"]["score_deviation"]
                for test_result in model_result["test_results"]
                for r in test_result["results"]
                if "evaluation" in r and "score_deviation" in r["evaluation"]
            ]
            avg_score_deviation = sum(score_deviations) / len(score_deviations) if score_deviations else 0

            lines.append(
                f"| {model} | {total_results} | {accuracy:.2%} | {avg_score_deviation:.2f} | {stats['average_time']:.2f}s |\n"
            )

        lines.append("\n---\n\n")

        # è¯¦ç»†ç»“æœ
        lines.append("## ğŸ“‹ è¯¦ç»†ç»“æœ\n\n")

        for model_result in self.results:
            model = model_result["model"]
            lines.append(f"### {model}\n\n")

            for test_result in model_result["test_results"][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªæµ‹è¯•ç”¨ä¾‹
                lines.append(f"#### æµ‹è¯•ç”¨ä¾‹: {test_result['test_case_id']}\n\n")
                lines.append(f"- ç›®æ ‡: {test_result['target']['country']} {test_result['target']['grade']} {test_result['target']['subject']}\n")
                lines.append(f"- è€—æ—¶: {test_result['execution_time']:.2f}ç§’\n\n")

                lines.append("| æ ‡é¢˜ | å®é™…è¯„åˆ† | æœŸæœ›è¯„åˆ† | åŒ¹é… |\n")
                lines.append("|------|---------|---------|------|\n")

                for result in test_result["results"]:
                    title = result["title"][:40] + "..." if len(result["title"]) > 40 else result["title"]
                    score = result.get("score", "N/A")
                    expected = result.get("expected_score", "N/A")
                    match = "âœ…" if result.get("success", False) else "âŒ"

                    lines.append(f"| {title} | {score} | {expected} | {match} |\n")

                lines.append("\n")

        # ä¿å­˜æŠ¥å‘Š
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        logger.info(f"âœ… æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
