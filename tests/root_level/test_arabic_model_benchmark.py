#!/usr/bin/env python3
"""
é˜¿æ‹‰ä¼¯è¯­ç†è§£èƒ½åŠ›å¯¹æ¯”æµ‹è¯•
å¯¹æ¯”ä¸åŒLLMæ¨¡å‹å¯¹é˜¿æ‹‰ä¼¯è¯­æ•™è‚²å†…å®¹çš„ç†è§£èƒ½åŠ›
"""

import os
import sys
import json
import time
from typing import Dict, List, Tuple
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from openai import OpenAI


class ArabicLanguageBenchmark:
    """é˜¿æ‹‰ä¼¯è¯­ç†è§£èƒ½åŠ›åŸºå‡†æµ‹è¯•"""

    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•"""
        self.api_key = os.getenv("INTERNAL_API_KEY")
        self.base_url = os.getenv("INTERNAL_API_BASE_URL", "https://hk-intra-paas.transsion.com/tranai-proxy/v1")

        if not self.api_key:
            print("âŒ è¯·è®¾ç½® INTERNAL_API_KEY ç¯å¢ƒå˜é‡")
            sys.exit(1)

        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

        # æµ‹è¯•ç”¨ä¾‹
        self.test_cases = [
            {
                "name": "å°å­¦äºŒå¹´çº§ - æ­£ç¡®",
                "title": "Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ù„Ù„ØµÙ Ø§Ù„Ø«Ø§Ù†ÙŠ Ø§Ø¨ØªØ¯Ø§Ø¦ÙŠ",
                "expected_grade": "Grade 2 (å°å­¦)",
                "expected_subject": "Mathematics",
                "should_match": True,
                "description": "å°å­¦äºŒå¹´çº§æ•°å­¦ - åº”è¯¥è¯†åˆ«ä¸ºåŒ¹é…"
            },
            {
                "name": "åˆä¸­äºŒå¹´çº§ - é”™è¯¯",
                "title": "Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø§Ù„ØµÙ Ø§Ù„Ø«Ø§Ù†ÙŠ Ù…ØªÙˆØ³Ø·",
                "expected_grade": "Grade 8 (åˆä¸­)",
                "expected_subject": "Mathematics",
                "should_match": False,
                "description": "åˆä¸­äºŒå¹´çº§æ•°å­¦ - ä¸åº”è¯¥åŒ¹é…å°å­¦äºŒå¹´çº§"
            },
            {
                "name": "åäºŒå¹´çº§ - é”™è¯¯",
                "title": "Ø´Ø±Ø­ Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø§Ù„ØµÙ Ø§Ù„Ø«Ø§Ù†ÙŠ Ø¹Ø´Ø±",
                "expected_grade": "Grade 12 (é«˜ä¸­)",
                "expected_subject": "Mathematics",
                "should_match": False,
                "description": "åäºŒå¹´çº§æ•°å­¦ - ä¸åº”è¯¥åŒ¹é…å°å­¦äºŒå¹´çº§"
            },
            {
                "name": "G2ç¼©å†™ - æ­£ç¡®",
                "title": "G2 ÙÙŠØ¯ÙŠÙˆ ÙƒØ±ØªÙˆÙ† Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª",
                "expected_grade": "Grade 2",
                "expected_subject": "Mathematics",
                "should_match": True,
                "description": "G2æ•°å­¦å¡é€šè§†é¢‘ - åº”è¯¥è¯†åˆ«ä¸ºäºŒå¹´çº§"
            },
            {
                "name": "çº¯é˜¿æ‹‰ä¼¯è¯­ - æ­£ç¡®",
                "title": "Ø¬Ù…ÙŠØ¹ Ø¯Ø±ÙˆØ³ Ù…Ù†Ù‡Ø§Ø¬ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø§Ù„ØµÙ Ø§Ù„Ø«Ø§Ù†ÙŠ",
                "expected_grade": "Grade 2",
                "expected_subject": "Mathematics",
                "should_match": True,
                "description": "äºŒå¹´çº§æ•°å­¦å®Œæ•´è¯¾ç¨‹ - åº”è¯¥è¯†åˆ«ä¸ºåŒ¹é…"
            }
        ]

        # è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨
        self.models_to_test = [
            # OpenAI ç³»åˆ—
            "gpt-5.2",
            "gpt-5-mini",
            "gpt-5.2-thinking-high",
            "gpt-4.1",

            # Gemini ç³»åˆ—
            "gemini-2.5-flash",
            "gemini-2.5-pro",

            # Claude ç³»åˆ—
            "claude-3-7-sonnet@20250219",

            # Ali ç³»åˆ—
            "qwen3-max",

            # TranAI ç³»åˆ—
            "tranai/deepseek-v3.1"
        ]

        self.results = []

    def test_grade_recognition(self, model: str, test_case: Dict) -> Dict:
        """æµ‹è¯•å¹´çº§è¯†åˆ«èƒ½åŠ›"""

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•™è‚²å†…å®¹åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹é˜¿æ‹‰ä¼¯è¯­æ ‡é¢˜ï¼Œæå–å¹´çº§ä¿¡æ¯ã€‚

æ ‡é¢˜: {test_case['title']}

è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
1. æ ‡é¢˜ä¸­çš„å¹´çº§æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆç”¨é˜¿æ‹‰ä¼¯è¯­æˆ–è‹±è¯­ï¼‰
2. è¿™ä¸ªå¹´çº§å¯¹åº”ä¸­å›½å­¦åˆ¶çš„å“ªä¸ªå¹´çº§ï¼Ÿï¼ˆä¾‹å¦‚ï¼šå°å­¦ä¸€å¹´çº§ã€åˆä¸­äºŒå¹´çº§ã€é«˜ä¸­ä¸‰å¹´çº§ï¼‰
3. å¦‚æœæ ‡é¢˜æ˜¯"Ø§Ù„ØµÙ Ø§Ù„Ø«Ø§Ù†ÙŠ"ï¼Œå®ƒæ˜¯äºŒå¹´çº§ã€åäºŒå¹´çº§è¿˜æ˜¯åˆäºŒï¼Ÿè¯·è¯´æ˜ç†ç”±ã€‚

è¯·ç”¨JSONæ ¼å¼å›ç­”ï¼š
{{
    "detected_grade_arabic": "æ£€æµ‹åˆ°çš„é˜¿æ‹‰ä¼¯è¯­å¹´çº§",
    "detected_grade_english": "å¯¹åº”çš„è‹±è¯­å¹´çº§",
    "chinese_equivalent": "å¯¹åº”ä¸­å›½å­¦åˆ¶",
    "confidence": 0.0-1.0,
    "reasoning": "åˆ¤æ–­ç†ç”±"
}}
"""

        try:
            start_time = time.time()
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¤šè¯­è¨€æ•™è‚²å†…å®¹åˆ†æå¸ˆï¼Œæ“…é•¿é˜¿æ‹‰ä¼¯è¯­ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                max_tokens=500,
                timeout=60
            )
            elapsed_time = time.time() - start_time

            response_text = response.choices[0].message.content

            # è§£æJSON
            import re
            json_match = re.search(r'\{[^}]+\}', response_text, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                result['elapsed_time'] = elapsed_time
                result['raw_response'] = response_text
                return result
            else:
                return {
                    "error": "æ— æ³•è§£æJSON",
                    "raw_response": response_text,
                    "elapsed_time": elapsed_time
                }

        except Exception as e:
            return {
                "error": str(e),
                "elapsed_time": time.time() - start_time
            }

    def test_scoring_accuracy(self, model: str, test_case: Dict) -> Dict:
        """æµ‹è¯•è¯„åˆ†å‡†ç¡®æ€§"""

        prompt = f"""è¯·ä¸ºä»¥ä¸‹æ•™è‚²èµ„æºè¯„åˆ†ï¼ˆ0-10åˆ†ï¼‰ï¼š

ã€ç›®æ ‡å¹´çº§ã€‘å°å­¦äºŒå¹´çº§ (Grade 2)
ã€ç›®æ ‡å­¦ç§‘ã€‘æ•°å­¦ (Mathematics)

ã€èµ„æºæ ‡é¢˜ã€‘{test_case['title']}

è¯„åˆ†æ ‡å‡†ï¼š
- å¹´çº§å®Œå…¨åŒ¹é…ï¼ˆå°å­¦äºŒå¹´çº§ï¼‰ï¼š3åˆ†
- å­¦ç§‘å®Œå…¨åŒ¹é…ï¼ˆæ•°å­¦ï¼‰ï¼š3åˆ†
- å¹´çº§ä¸ç¬¦ï¼ˆå¦‚åˆä¸­ã€é«˜ä¸­ï¼‰ï¼š0åˆ†

è¯·è¿”å›JSONï¼š
{{
    "score": 0.0-10.0,
    "grade_match": true/false,
    "subject_match": true/false,
    "reasoning": "è¯„åˆ†ç†ç”±"
}}
"""

        try:
            start_time = time.time()
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯æ•™è‚²å†…å®¹è¯„åˆ†ä¸“å®¶ï¼Œæ“…é•¿é˜¿æ‹‰ä¼¯è¯­ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                max_tokens=300,
                timeout=60
            )
            elapsed_time = time.time() - start_time

            response_text = response.choices[0].message.content

            # è§£æJSON
            import re
            json_match = re.search(r'\{[^}]+\}', response_text, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                result['elapsed_time'] = elapsed_time

                # åˆ¤æ–­è¯„åˆ†æ˜¯å¦æ­£ç¡®
                expected_low_score = not test_case['should_match']
                actual_low_score = result.get('score', 10) < 6.0

                if expected_low_score:
                    result['correct'] = actual_low_score  # åº”è¯¥ä½åˆ†ï¼Œå®é™…ä½åˆ† = æ­£ç¡®
                else:
                    result['correct'] = not actual_low_score and result.get('grade_match', False)  # åº”è¯¥é«˜åˆ†ï¼Œå®é™…é«˜åˆ† = æ­£ç¡®

                result['raw_response'] = response_text
                return result
            else:
                return {
                    "error": "æ— æ³•è§£æJSON",
                    "raw_response": response_text,
                    "elapsed_time": elapsed_time,
                    "correct": False
                }

        except Exception as e:
            return {
                "error": str(e),
                "elapsed_time": time.time() - start_time,
                "correct": False
            }

    def run_benchmark(self):
        """è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•"""

        print("=" * 120)
        print("ğŸ§ª é˜¿æ‹‰ä¼¯è¯­ç†è§£èƒ½åŠ›å¯¹æ¯”æµ‹è¯•")
        print("=" * 120)
        print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ”§ API: {self.base_url}")
        print(f"ğŸ“Š æµ‹è¯•æ¨¡å‹æ•°: {len(self.models_to_test)}")
        print(f"ğŸ“ æµ‹è¯•ç”¨ä¾‹æ•°: {len(self.test_cases)}")
        print("=" * 120)

        for model in self.models_to_test:
            print(f"\n{'=' * 120}")
            print(f"ğŸ¤– æµ‹è¯•æ¨¡å‹: {model}")
            print(f"{'=' * 120}")

            model_results = {
                "model": model,
                "tests": [],
                "total_correct": 0,
                "total_tests": 0,
                "avg_time": 0
            }

            total_time = 0

            for i, test_case in enumerate(self.test_cases, 1):
                print(f"\n  [{i}/{len(self.test_cases)}] æµ‹è¯•: {test_case['name']}")
                print(f"  æ ‡é¢˜: {test_case['title']}")
                print(f"  æœŸæœ›: {test_case['expected_grade']} - {'åº”è¯¥åŒ¹é…' if test_case['should_match'] else 'ä¸åº”è¯¥åŒ¹é…'}")

                # æµ‹è¯•1: å¹´çº§è¯†åˆ«
                print(f"  ğŸ“ æµ‹è¯•å¹´çº§è¯†åˆ«...")
                recognition_result = self.test_grade_recognition(model, test_case)

                # æµ‹è¯•2: è¯„åˆ†å‡†ç¡®æ€§
                print(f"  ğŸ¯ æµ‹è¯•è¯„åˆ†å‡†ç¡®æ€§...")
                scoring_result = self.test_scoring_accuracy(model, test_case)

                # è®°å½•ç»“æœ
                test_result = {
                    "test_case": test_case['name'],
                    "recognition": recognition_result,
                    "scoring": scoring_result
                }

                model_results['tests'].append(test_result)

                # åˆ¤æ–­æ˜¯å¦æ­£ç¡®
                is_correct = (
                    scoring_result.get('correct', False) and
                    not scoring_result.get('error')
                )

                if is_correct:
                    model_results['total_correct'] += 1
                    print(f"  âœ… é€šè¿‡ - è€—æ—¶: {scoring_result.get('elapsed_time', 0):.2f}s")
                else:
                    print(f"  âŒ å¤±è´¥ - åŸå› : {scoring_result.get('error', 'è¯„åˆ†ä¸æ­£ç¡®')[:60]}")

                model_results['total_tests'] += 1
                total_time += scoring_result.get('elapsed_time', 0)

            # è®¡ç®—ç»Ÿè®¡
            model_results['avg_time'] = total_time / len(self.test_cases)
            model_results['accuracy'] = model_results['total_correct'] / model_results['total_tests'] if model_results['total_tests'] > 0 else 0

            print(f"\n  ğŸ“Š {model} ç»Ÿè®¡:")
            print(f"     å‡†ç¡®ç‡: {model_results['accuracy']:.1%}")
            print(f"     æ­£ç¡®æ•°: {model_results['total_correct']}/{model_results['total_tests']}")
            print(f"     å¹³å‡è€—æ—¶: {model_results['avg_time']:.2f}s")

            self.results.append(model_results)

        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary_report()

    def generate_summary_report(self):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""

        print("\n\n" + "=" * 120)
        print("ğŸ“Š æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
        print("=" * 120)

        # æŒ‰å‡†ç¡®ç‡æ’åº
        sorted_results = sorted(self.results, key=lambda x: x['accuracy'], reverse=True)

        print(f"\n{'æ¨¡å‹':<35} {'å‡†ç¡®ç‡':<10} {'æ­£ç¡®/æ€»æ•°':<12} {'å¹³å‡è€—æ—¶':<10} {'æ’å':<5}")
        print("-" * 120)

        for i, result in enumerate(sorted_results, 1):
            model = result['model']
            accuracy = result['accuracy']
            correct = result['total_correct']
            total = result['total_tests']
            avg_time = result['avg_time']

            print(f"{model:<35} {accuracy:>8.1%} {correct:>3}/{total:<8} {avg_time:>8.2f}s   #{i}")

        # æ¨èæœ€ä½³æ¨¡å‹
        print("\n" + "=" * 120)
        print("ğŸ’¡ æ¨è")

        best_accuracy = sorted_results[0]
        fastest = min(self.results, key=lambda x: x['avg_time'])

        print(f"ğŸ† å‡†ç¡®ç‡æœ€é«˜: {best_accuracy['model']} ({best_accuracy['accuracy']:.1%})")
        print(f"âš¡ é€Ÿåº¦æœ€å¿«: {fastest['model']} ({fastest['avg_time']:.2f}s)")

        # å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦
        best_balance = None
        best_score = -1

        for result in self.results:
            # ç»¼åˆè¯„åˆ† = å‡†ç¡®ç‡ * 2 - (å¹³å‡è€—æ—¶ / 10)
            score = result['accuracy'] * 2 - (result['avg_time'] / 10)
            if score > best_score:
                best_score = score
                best_balance = result

        if best_balance:
            print(f"â­ æ€§ä»·æ¯”æœ€é«˜: {best_balance['model']} (å‡†ç¡®ç‡ {best_balance['accuracy']:.1%}, è€—æ—¶ {best_balance['avg_time']:.2f}s)")

        print("\n" + "=" * 120)
        print("â“ å»ºè®®")

        if best_accuracy['accuracy'] >= 0.8:
            print(f"âœ… æ‰¾åˆ°é«˜å‡†ç¡®ç‡æ¨¡å‹ ({best_accuracy['model']})ï¼ŒçŸ¥è¯†åº“å¯èƒ½ä¸éœ€è¦")
        elif best_accuracy['accuracy'] >= 0.6:
            print(f"âš ï¸ å‡†ç¡®ç‡ä¸­ç­‰ ({best_accuracy['model']})ï¼ŒçŸ¥è¯†åº“å¯ä»¥ä½œä¸ºè¾…åŠ©")
        else:
            print(f"âŒ æ‰€æœ‰æ¨¡å‹å‡†ç¡®ç‡éƒ½è¾ƒä½ (<60%)ï¼ŒçŸ¥è¯†åº“æ–¹æ¡ˆæ˜¯å¿…è¦çš„")

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        self.save_detailed_report(sorted_results)

    def save_detailed_report(self, sorted_results):
        """ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶"""

        report = {
            "timestamp": datetime.now().isoformat(),
            "api_base_url": self.base_url,
            "summary": {
                "best_accuracy": {
                    "model": sorted_results[0]['model'],
                    "accuracy": sorted_results[0]['accuracy']
                },
                "fastest": min(self.results, key=lambda x: x['avg_time'])['model']
            },
            "detailed_results": sorted_results
        }

        output_file = "arabic_benchmark_report.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")


if __name__ == "__main__":
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    é˜¿æ‹‰ä¼¯è¯­ç†è§£èƒ½åŠ›å¯¹æ¯”æµ‹è¯•                                  â•‘
â•‘                  Arabic Language Understanding Benchmark                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ æµ‹è¯•ç›®æ ‡ï¼šå¯¹æ¯”ä¸åŒLLMæ¨¡å‹å¯¹é˜¿æ‹‰ä¼¯è¯­æ•™è‚²å†…å®¹çš„ç†è§£èƒ½åŠ›
ğŸ“Š æµ‹è¯•å†…å®¹ï¼šå¹´çº§è¯†åˆ«ã€è¯„åˆ†å‡†ç¡®æ€§ã€å“åº”é€Ÿåº¦
ğŸ”§ ä½¿ç”¨APIï¼šå…¬å¸å†…éƒ¨ TranAI API
    """)

    benchmark = ArabicLanguageBenchmark()

    try:
        benchmark.run_benchmark()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
