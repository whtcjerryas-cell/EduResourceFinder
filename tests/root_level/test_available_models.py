#!/usr/bin/env python3
"""
æµ‹è¯•å…¬å¸å†…éƒ¨APIæ”¯æŒçš„æ¨¡å‹ï¼Œæ‰¾å‡ºé€‚åˆå¿«é€Ÿè¯„ä¼°çš„æ¨¡å‹
æµ‹è¯•å†…å®¹ï¼š
1. åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
2. æµ‹è¯•æ¯ä¸ªæ¨¡å‹çš„å“åº”é€Ÿåº¦
3. æ¨èé€‚åˆæœç´¢è¯„ä¼°çš„å¿«é€Ÿæ¨¡å‹
"""

import os
import sys
import time
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from llm_client import InternalAPIClient, AIBuildersAPIClient, UnifiedLLMClient

# æµ‹è¯•æç¤ºè¯ï¼ˆæœç´¢æ¨èç†ç”±ç”Ÿæˆåœºæ™¯ï¼‰
TEST_PROMPT = """è¯·ä¸ºä»¥ä¸‹æœç´¢ç»“æœç”Ÿæˆç®€æ´çš„æ¨èç†ç”±ï¼ˆ20-50å­—ï¼‰ï¼š

æ ‡é¢˜: äºŒå¹´çº§æ•°å­¦åŠ å‡æ³•æ•™å­¦è§†é¢‘
æè¿°: è¿™ä¸ªè§†é¢‘é€‚åˆå°å°¼äºŒå¹´çº§å­¦ç”Ÿå­¦ä¹ æ•°å­¦

è¯·è¿”å›ä¸€ä¸ªJSONæ•°ç»„æ ¼å¼ï¼š
["æ¨èç†ç”±"]
"""

# ç®€åŒ–æµ‹è¯•æç¤ºè¯ï¼ˆæ›´çŸ­ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
QUICK_TEST_PROMPT = """ç”Ÿæˆä¸€ä¸ªJSONæ•°ç»„ï¼ŒåŒ…å«ä¸€æ¡20å­—çš„æ¨èç†ç”±ï¼š
["é€‚åˆå°å°¼äºŒå¹´çº§å­¦ç”Ÿå­¦ä¹ åŸºç¡€åŠ å‡æ³•"]"""


def test_internal_api_models():
    """æµ‹è¯•å…¬å¸å†…éƒ¨APIæ”¯æŒçš„æ¨¡å‹"""
    print("=" * 80)
    print("ğŸ¢ æµ‹è¯•å…¬å¸å†…éƒ¨APIæ¨¡å‹")
    print("=" * 80)
    print()

    # å¯èƒ½çš„æ¨¡å‹åˆ—è¡¨ï¼ˆæ ¹æ®OpenAIå…¼å®¹æ¥å£ï¼‰
    # é‡ç‚¹æµ‹è¯•å¯èƒ½è¾ƒå¿«çš„æ¨¡å‹
    models_to_test = [
        "gpt-4o-mini",      # è½»é‡ç‰ˆGPT-4oï¼ˆé€šå¸¸æœ€å¿«ï¼‰
        "gpt-4o",           # å½“å‰é»˜è®¤
        "gpt-3.5-turbo",    # GPT-3.5 Turboï¼ˆç»å…¸å¿«é€Ÿæ¨¡å‹ï¼‰
    ]

    results = []

    for model in models_to_test:
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹: {model}")
        print("-" * 60)

        try:
            client = InternalAPIClient()

            start_time = time.time()

            response = client.call_llm(
                prompt=QUICK_TEST_PROMPT,
                max_tokens=100,
                temperature=0.3,
                model=model
            )

            elapsed_time = time.time() - start_time

            print(f"âœ… æˆåŠŸï¼å“åº”æ—¶é—´: {elapsed_time:.2f} ç§’")
            print(f"ğŸ“ å“åº”å†…å®¹: {response[:100]}...")

            results.append({
                "model": model,
                "success": True,
                "time": elapsed_time,
                "response_length": len(response)
            })

        except Exception as e:
            print(f"âŒ å¤±è´¥: {str(e)[:100]}")
            results.append({
                "model": model,
                "success": False,
                "error": str(e)[:100]
            })

    # ç»“æœæ±‡æ€»
    print("\n" + "=" * 80)
    print("ğŸ“Š å…¬å¸å†…éƒ¨APIæ¨¡å‹æµ‹è¯•ç»“æœ")
    print("=" * 80)
    print()

    successful = [r for r in results if r['success']]
    if successful:
        print("âœ… æˆåŠŸçš„æ¨¡å‹:")
        successful_sorted = sorted(successful, key=lambda x: x['time'])
        for i, r in enumerate(successful_sorted, 1):
            speed_icon = "ğŸš€" if r['time'] < 5 else "âš¡" if r['time'] < 10 else "ğŸ¢"
            print(f"  {i}. [{speed_icon}] {r['model']:40s} - {r['time']:6.2f}ç§’")

    failed = [r for r in results if not r['success']]
    if failed:
        print("\nâŒ å¤±è´¥çš„æ¨¡å‹:")
        for r in failed:
            print(f"  - {r['model']}: {r['error']}")

    return results


def test_ai_builders_models():
    """æµ‹è¯•AI Builders APIæ”¯æŒçš„æ¨¡å‹"""
    print("\n" + "=" * 80)
    print("ğŸ”§ æµ‹è¯•AI Builders APIæ¨¡å‹")
    print("=" * 80)
    print()

    # AI Buildersæ”¯æŒçš„æ¨¡å‹ - é‡ç‚¹æµ‹è¯•å¿«é€Ÿæ¨¡å‹
    models_to_test = [
        "deepseek",         # å½“å‰é»˜è®¤
        "gemini-2.0-flash-exp", # Gemini Flash 2.0ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        "gemini-1.5-flash", # Gemini 1.5 Flash
        "grok-4-fast",      # Grokå¿«é€Ÿç‰ˆ
    ]

    results = []

    for model in models_to_test:
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹: {model}")
        print("-" * 60)

        try:
            client = AIBuildersAPIClient()

            start_time = time.time()

            response = client.call_llm(
                prompt=QUICK_TEST_PROMPT,
                max_tokens=100,
                temperature=0.3,
                model=model
            )

            elapsed_time = time.time() - start_time

            print(f"âœ… æˆåŠŸï¼å“åº”æ—¶é—´: {elapsed_time:.2f} ç§’")
            print(f"ğŸ“ å“åº”å†…å®¹: {response[:100]}...")

            results.append({
                "model": model,
                "success": True,
                "time": elapsed_time,
                "response_length": len(response)
            })

        except Exception as e:
            print(f"âŒ å¤±è´¥: {str(e)[:100]}")
            results.append({
                "model": model,
                "success": False,
                "error": str(e)[:100]
            })

    # ç»“æœæ±‡æ€»
    print("\n" + "=" * 80)
    print("ğŸ“Š AI Builders APIæ¨¡å‹æµ‹è¯•ç»“æœ")
    print("=" * 80)
    print()

    successful = [r for r in results if r['success']]
    if successful:
        print("âœ… æˆåŠŸçš„æ¨¡å‹:")
        successful_sorted = sorted(successful, key=lambda x: x['time'])
        for i, r in enumerate(successful_sorted, 1):
            speed_icon = "ğŸš€" if r['time'] < 5 else "âš¡" if r['time'] < 10 else "ğŸ¢"
            print(f"  {i}. [{speed_icon}] {r['model']:40s} - {r['time']:6.2f}ç§’")

    failed = [r for r in results if not r['success']]
    if failed:
        print("\nâŒ å¤±è´¥çš„æ¨¡å‹:")
        for r in failed:
            print(f"  - {r['model']}: {r['error']}")

    return results


def generate_recommendations():
    """ç”Ÿæˆæ¨èå»ºè®®"""
    print("\n" + "=" * 80)
    print("ğŸ’¡ æ¨èå»ºè®®")
    print("=" * 80)
    print()

    recommendations = [
        {
            "åœºæ™¯": "æœç´¢æ¨èç†ç”±ç”Ÿæˆï¼ˆéœ€è¦å¿«é€Ÿï¼‰",
            "æ¨èæ¨¡å‹": [
                "1. gemini-2.0-flash æˆ– gemini-1.5-flash - Gemini Flashç³»åˆ—ä¸“é—¨ä¸ºå¿«é€Ÿæ¨ç†ä¼˜åŒ–",
                "2. deepseek - æ€§ä»·æ¯”é«˜ï¼Œé€Ÿåº¦è¾ƒå¿«",
                "3. gpt-4o-mini - å¦‚æœå…¬å¸å†…éƒ¨APIæ”¯æŒï¼Œè¿™æ˜¯æœ€å¿«é€Ÿçš„é€‰æ‹©",
                "4. grok-4-fast - ä»£ç ä¸­å·²æœ‰ä½¿ç”¨ï¼Œåº”è¯¥æ˜¯å¿«é€Ÿç‰ˆæœ¬"
            ],
            "ç†ç”±": "æœç´¢æ—¶éœ€è¦å¿«é€Ÿç”Ÿæˆæ¨èç†ç”±ï¼Œåº”è¯¥ä¼˜å…ˆä½¿ç”¨å“åº”æ—¶é—´<10ç§’çš„æ¨¡å‹"
        },
        {
            "åœºæ™¯": "è§†é¢‘æ·±åº¦è¯„ä¼°ï¼ˆå¯ä»¥æ…¢ä¸€äº›ï¼‰",
            "æ¨èæ¨¡å‹": [
                "1. gpt-4o - è§†è§‰ç†è§£èƒ½åŠ›å¼º",
                "2. gemini-2.5-pro - å¤šæ¨¡æ€èƒ½åŠ›å¥½",
                "3. deepseek - å¤‡ç”¨æ–¹æ¡ˆ"
            ],
            "ç†ç”±": "æ·±åº¦è¯„ä¼°å¯¹è´¨é‡è¦æ±‚æ›´é«˜ï¼Œå¯ä»¥ä½¿ç”¨æ›´æ…¢ä½†æ›´å‡†ç¡®çš„æ¨¡å‹"
        },
        {
            "åœºæ™¯": "æœç´¢ç­–ç•¥ç”Ÿæˆ",
            "æ¨èæ¨¡å‹": [
                "1. deepseek - å½“å‰ä½¿ç”¨ï¼Œæˆæœ¬æ•ˆç›Šå¥½",
                "2. gemini-2.5-pro - å¤‡ç”¨æ–¹æ¡ˆ"
            ],
            "ç†ç”±": "ç­–ç•¥ç”Ÿæˆä¸éœ€è¦è§†è§‰èƒ½åŠ›ï¼Œä½¿ç”¨æ–‡æœ¬æ¨¡å‹å³å¯"
        }
    ]

    for rec in recommendations:
        print(f"ğŸ“Œ {rec['åœºæ™¯']}")
        print(f"æ¨èæ¨¡å‹:")
        for model in rec['æ¨èæ¨¡å‹']:
            print(f"  {model}")
        print(f"ç†ç”±: {rec['ç†ç”±']}")
        print()


if __name__ == "__main__":
    print()
    print("ğŸ” å¼€å§‹æµ‹è¯•å¯ç”¨æ¨¡å‹...")
    print()

    # æµ‹è¯•å…¬å¸å†…éƒ¨API
    internal_results = test_internal_api_models()

    # æµ‹è¯•AI Builders API
    ai_builders_results = test_ai_builders_models()

    # ç”Ÿæˆæ¨è
    generate_recommendations()

    print("=" * 80)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)
    print()
    print("ğŸ“ æ€»ç»“:")
    print("  1. æŸ¥çœ‹ä¸Šè¿°æµ‹è¯•ç»“æœï¼Œæ‰¾åˆ°å“åº”æœ€å¿«çš„æ¨¡å‹")
    print("  2. æ ¹æ®ä¸åŒåœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨¡å‹")
    print("  3. å»ºè®®åœ¨é…ç½®æ–‡ä»¶ä¸­é…ç½®å¿«é€Ÿæ¨¡å‹ç”¨äºæœç´¢è¯„ä¼°")
    print()
