"""
éªŒè¯å·¥å…· - éªŒè¯å¹´çº§åŒ¹é…å’ŒURLè´¨é‡

éµå¾ª agent-native åŸåˆ™ï¼š
- åŸå­å·¥å…·ï¼šåªè´Ÿè´£éªŒè¯åˆ¤æ–­ï¼Œä¸ç¼–ç ä¸šåŠ¡é€»è¾‘
- ä¸°å¯Œè¾“å‡ºï¼šè¿”å›å®Œæ•´éªŒè¯ä¿¡æ¯ä¾›Agentå†³ç­–
- å¯é…ç½®ï¼šé€šè¿‡é…ç½®æ–‡ä»¶å®šä¹‰è§„åˆ™
"""

import re
from typing import Dict, List, Any, Optional
from urllib.parse import urlparse
from logger_utils import get_logger
from .config_tools import read_country_config

logger = get_logger('validation_tools')


async def validate_grade_match(
    target_grade: str,
    identified_grade: str,
    country_code: str
) -> Dict[str, Any]:
    """
    éªŒè¯å¹´çº§æ˜¯å¦åŒ¹é…

    Args:
        target_grade: ç›®æ ‡å¹´çº§ï¼ˆå¯ä»¥æ˜¯å¹´çº§IDã€ä¸­æ–‡åã€æœ¬åœ°åï¼‰
        identified_grade: è¯†åˆ«å‡ºçš„å¹´çº§ï¼ˆåŒä¸Šï¼‰
        country_code: å›½å®¶ä»£ç 

    Returns:
        {
            "success": True/False,
            "data": {
                "match": True/False,
                "target_grade_id": "1",
                "identified_grade_id": "6",
                "confidence": "high",
                "reason": "å¹´çº§åŒ¹é…" / "å¹´çº§ä¸åŒ¹é…"
            },
            "text": "å¹´çº§åŒ¹é…ï¼šä¸€å¹´çº§ (Kelas 1)"
        }

    ç¤ºä¾‹ï¼š
        >>> result = await validate_grade_match("ä¸€å¹´çº§", "Kelas 1", "ID")
        >>> print(result['data'])
        {'match': True, 'target_grade_id': '1', 'identified_grade_id': '1', 'confidence': 'high', 'reason': 'å¹´çº§åŒ¹é…'}

        >>> result = await validate_grade_match("Kelas 1", "Kelas 6", "ID")
        >>> print(result['data'])
        {'match': False, 'target_grade_id': '1', 'identified_grade_id': '6', 'confidence': 'high', 'reason': 'å¹´çº§ä¸åŒ¹é…'}
    """
    try:
        # 1. è¯»å–å›½å®¶é…ç½®
        config_result = await read_country_config(country_code)

        if not config_result["success"]:
            return {
                "success": False,
                "data": None,
                "text": f"æ— æ³•è¯»å–é…ç½®ï¼š{config_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
            }

        config = config_result["data"]
        grades = config.get("grades", [])

        # 2. åˆ›å»ºæŸ¥æ‰¾æ˜ å°„ï¼ˆæ”¯æŒIDã€ä¸­æ–‡åã€æœ¬åœ°åæŸ¥æ‰¾ï¼‰
        grade_lookup = {}
        for grade in grades:
            grade_id = grade["grade_id"]
            grade_lookup[grade_id] = grade
            grade_lookup[grade["zh_name"]] = grade
            grade_lookup[grade["local_name"]] = grade
            # ä¹Ÿæ”¯æŒè‹±æ–‡åæŸ¥æ‰¾
            if "en_name" in grade:
                grade_lookup[grade["en_name"]] = grade

        # 3. æŸ¥æ‰¾ç›®æ ‡å¹´çº§å’Œè¯†åˆ«å¹´çº§
        target_grade_info = grade_lookup.get(target_grade)
        identified_grade_info = grade_lookup.get(identified_grade)

        if not target_grade_info:
            return {
                "success": False,
                "data": None,
                "text": f"æ— æ³•è¯†åˆ«ç›®æ ‡å¹´çº§ï¼š{target_grade}ï¼Œå¯ç”¨å¹´çº§ï¼š{', '.join([g['zh_name'] for g in grades[:5]])}..."
            }

        if not identified_grade_info:
            return {
                "success": False,
                "data": None,
                "text": f"æ— æ³•è¯†åˆ«å¹´çº§ï¼š{identified_grade}ï¼Œå¯ç”¨å¹´çº§ï¼š{', '.join([g['zh_name'] for g in grades[:5]])}..."
            }

        # 4. æ¯”è¾ƒå¹´çº§IDï¼ˆæœ€å¯é çš„åŒ¹é…æ–¹å¼ï¼‰
        target_grade_id = target_grade_info["grade_id"]
        identified_grade_id = identified_grade_info["grade_id"]

        is_match = target_grade_id == identified_grade_id

        if is_match:
            return {
                "success": True,
                "data": {
                    "match": True,
                    "target_grade_id": target_grade_id,
                    "identified_grade_id": identified_grade_id,
                    "target_grade_name": target_grade_info["zh_name"],
                    "identified_grade_name": identified_grade_info["zh_name"],
                    "target_grade_local": target_grade_info["local_name"],
                    "identified_grade_local": identified_grade_info["local_name"],
                    "confidence": "high",
                    "reason": "å¹´çº§åŒ¹é…"
                },
                "text": f"å¹´çº§åŒ¹é…ï¼š{target_grade_info['zh_name']} ({target_grade_info['local_name']})"
            }
        else:
            return {
                "success": True,
                "data": {
                    "match": False,
                    "target_grade_id": target_grade_id,
                    "identified_grade_id": identified_grade_id,
                    "target_grade_name": target_grade_info["zh_name"],
                    "identified_grade_name": identified_grade_info["zh_name"],
                    "target_grade_local": target_grade_info["local_name"],
                    "identified_grade_local": identified_grade_info["local_name"],
                    "confidence": "high",
                    "reason": "å¹´çº§ä¸åŒ¹é…"
                },
                "text": f"å¹´çº§ä¸åŒ¹é…ï¼šç›®æ ‡{target_grade_info['zh_name']} ({target_grade_info['local_name']})ï¼Œæ ‡é¢˜{identified_grade_info['zh_name']} ({identified_grade_info['local_name']})"
            }

    except Exception as e:
        logger.error(f"éªŒè¯å¹´çº§åŒ¹é…å¤±è´¥ï¼š{str(e)}")
        return {
            "success": False,
            "error": str(e),
            "data": None,
            "text": f"éªŒè¯å¤±è´¥ï¼š{str(e)}"
        }


async def validate_url_quality(url: str, title: str = "") -> Dict[str, Any]:
    """
    éªŒè¯URLæ¥æºè´¨é‡

    Args:
        url: èµ„æºURL
        title: èµ„æºæ ‡é¢˜ï¼ˆå¯é€‰ï¼Œç”¨äºè¾…åŠ©åˆ¤æ–­ï¼‰

    Returns:
        {
            "success": True/False,
            "data": {
                "quality": "high" / "medium" / "low",
                "reason": "trusted_platform" / "unknown_domain" / "social_media",
                "filter": True/False,
                "score_adjustment": 1.5 / -8.0
            },
            "text": "æ¨èï¼šæ¥æºæ˜¯YouTube" / "ä¸æ¨èï¼šæ¥æºæ˜¯ç¤¾äº¤åª’ä½“"
        }

    ç¤ºä¾‹ï¼š
        >>> result = await validate_url_quality("https://www.youtube.com/watch?v=xxx", "Math Tutorial")
        >>> print(result['data'])
        {'quality': 'high', 'reason': 'trusted_platform', 'filter': False, 'score_adjustment': 1.5}

        >>> result = await validate_url_quality("https://www.facebook.com/posts/xxx", "Video")
        >>> print(result['data'])
        {'quality': 'low', 'reason': 'social_media', 'filter': True, 'score_adjustment': -8.0}
    """
    try:
        logger.debug(f"[ğŸ” URLéªŒè¯å¼€å§‹] URL={url[:80]}..., æ ‡é¢˜={title[:50] if title else 'N/A'}")

        if not url:
            logger.warning(f"[âŒ URLéªŒè¯] URLä¸ºç©º")
            return {
                "success": False,
                "data": None,
                "text": "URLä¸ºç©º"
            }

        # 1. è§£æURLè·å–åŸŸå
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()

            # ç§»é™¤ www. å‰ç¼€
            if domain.startswith("www."):
                domain = domain[4:]

            logger.debug(f"[âœ… URLè§£æ] åŸŸå={domain}")

        except Exception as e:
            logger.error(f"[âŒ URLè§£æå¤±è´¥] {str(e)}")
            return {
                "success": False,
                "data": None,
                "text": f"URLè§£æå¤±è´¥ï¼š{str(e)}"
            }

        # 2. é»‘åå•æ£€æŸ¥ï¼ˆç¤¾äº¤åª’ä½“ã€éæ•™è‚²å†…å®¹ï¼‰
        blacklist = [
            # ç¤¾äº¤åª’ä½“
            'facebook.com', 'fb.com', 'fb.watch',
            'instagram.com', 'instagr.am',
            'twitter.com', 'x.com',
            'tiktok.com',
            'twitch.tv',
            'vk.com',
            'telegram.org', 't.me',
            'whatsapp.com',

            # çŸ­é“¾æ¥æœåŠ¡
            'bit.ly', 'bitly.com',
            'tinyurl.com',
            'short.link',
            'goo.gl',

            # éæ•™è‚²å†…å®¹ï¼ˆç”µå•†ã€æ¸¸æˆç­‰ï¼‰
            'amazon.com', 'amazon.co.', 'amazon.cn',
            'ebay.com', 'ebay.co.',
            'aliexpress.com',
            'taobao.com', 'tmall.com',
            'steam.com',
            'epicgames.com',
        ]

        logger.debug(f"[ğŸ” é»‘åå•æ£€æŸ¥] æ£€æŸ¥åŸŸå {domain} æ˜¯å¦åœ¨é»‘åå•ä¸­...")

        for blacklist_domain in blacklist:
            if blacklist_domain in domain:
                logger.warning(f"[âŒ é»‘åå•åŒ¹é…] åŸŸå {domain} åŒ¹é…é»‘åå•è§„åˆ™: {blacklist_domain}")
                return {
                    "success": True,
                    "data": {
                        "quality": "low",
                        "reason": "blacklist",
                        "domain": domain,
                        "filter": True,
                        "score_adjustment": -8.0,
                        "matched_rule": blacklist_domain
                    },
                    "text": f"ä¸æ¨èï¼šæ¥æºåœ¨é»‘åå•ä¸­ ({domain} - {blacklist_domain})"
                }

        logger.debug(f"[âœ… é»‘åå•æ£€æŸ¥] åŸŸå {domain} ä¸åœ¨é»‘åå•ä¸­")

        # 3. ä¿¡ä»»åˆ—è¡¨æ£€æŸ¥ï¼ˆæ•™è‚²å¹³å°ï¼‰
        trusted = [
            'youtube.com', 'youtu.be', 'youtube-nocookie.com',
            'vimeo.com',
            'drive.google.com', 'docs.google.com',
            'slideshare.net',
            'scribd.com',
            'academia.edu',
            'researchgate.net',
            'teachertube.com',
            'khanacademy.org',
            'coursera.org',
            'edx.org',
            'udemy.com',
            'skillshare.com',
        ]

        logger.debug(f"[ğŸ” ä¿¡ä»»åˆ—è¡¨æ£€æŸ¥] æ£€æŸ¥åŸŸå {domain} æ˜¯å¦åœ¨ä¿¡ä»»åˆ—è¡¨ä¸­...")

        for trusted_domain in trusted:
            if trusted_domain in domain:
                logger.info(f"[âœ… ä¿¡ä»»åˆ—è¡¨åŒ¹é…] åŸŸå {domain} åŒ¹é…ä¿¡ä»»è§„åˆ™: {trusted_domain}")

                # YouTubeé¢å¤–æ£€æŸ¥
                if 'youtube' in domain:
                    # YouTubeæ’­æ”¾åˆ—è¡¨åŠ åˆ†
                    if 'playlist' in url.lower():
                        logger.info(f"[ğŸ¬ YouTubeæ’­æ”¾åˆ—è¡¨] {url[:80]}...")
                        return {
                            "success": True,
                            "data": {
                                "quality": "high",
                                "reason": "youtube_playlist",
                                "domain": domain,
                                "filter": False,
                                "score_adjustment": 2.0,
                                "matched_rule": trusted_domain
                            },
                            "text": f"å¼ºçƒˆæ¨èï¼šæ¥æºæ˜¯YouTubeæ’­æ”¾åˆ—è¡¨ ({domain})"
                        }
                    # å•ä¸ªYouTubeè§†é¢‘
                    logger.info(f"[ğŸ¬ YouTubeè§†é¢‘] {url[:80]}...")
                    return {
                        "success": True,
                        "data": {
                            "quality": "high",
                            "reason": "trusted_platform",
                            "domain": domain,
                            "filter": False,
                            "score_adjustment": 1.5,
                            "matched_rule": trusted_domain
                        },
                        "text": f"æ¨èï¼šæ¥æºæ˜¯YouTube ({domain})"
                    }

                # å…¶ä»–ä¿¡ä»»å¹³å°
                logger.info(f"[âœ… å¯ä¿¡å¹³å°] åŸŸå {domain} æ˜¯å¯ä¿¡æ•™è‚²å¹³å°")
                return {
                    "success": True,
                    "data": {
                        "quality": "high",
                        "reason": "trusted_platform",
                        "domain": domain,
                        "filter": False,
                        "score_adjustment": 1.0,
                        "matched_rule": trusted_domain
                    },
                    "text": f"æ¨èï¼šæ¥æºæ˜¯å¯ä¿¡å¹³å° ({domain})"
                }

        logger.debug(f"[âŒ ä¿¡ä»»åˆ—è¡¨æ£€æŸ¥] åŸŸå {domain} ä¸åœ¨ä¿¡ä»»åˆ—è¡¨ä¸­")

        # 4. åŸºäºæ ‡é¢˜çš„è¡¥å……åˆ¤æ–­ï¼ˆå¦‚æœæœ‰æ ‡é¢˜ï¼‰
        if title:
            title_lower = title.lower()
            logger.debug(f"[ğŸ” æ ‡é¢˜å…³é”®è¯æ£€æŸ¥] æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«æ˜æ˜¾æ— å…³å†…å®¹...")

            # âœ… æ–°å¢ï¼šæ˜æ˜¾æ— å…³å†…å®¹æ£€æŸ¥ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼Œå¿…é¡»åœ¨spam_keywordsä¹‹å‰ï¼‰
            irrelevant_categories = {
                'automotive': ['rivian', 'tesla', 'ford', 'bmw', 'mercedes', 'toyota', 'honda',
                              'car', 'automotive', 'vehicle', 'truck', 'suv'],
                'music': ['drum', 'drums', 'guitar', 'piano', 'violin', 'instrument',
                         'music library', 'audio', 'band', 'orchestra'],
                'gaming': ['game', 'gaming', 'gameplay', 'streamer', 'twitch', 'steam',
                          'esport', 'console', 'playstation', 'xbox'],
                'shopping': ['shop', 'store', 'buy', 'purchase', 'price', 'sale', 'discount'],
                'news_general': ['news', 'breaking news', 'latest updates', 'rumors', 'gossip'],
            }

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ— å…³å†…å®¹å…³é”®è¯
            for category, keywords in irrelevant_categories.items():
                matched_keywords = [kw for kw in keywords if kw in title_lower]
                if matched_keywords:
                    logger.warning(f"[âŒ æ˜æ˜¾æ— å…³å†…å®¹] æ ‡é¢˜åŒ…å« {category} å…³é”®è¯: {matched_keywords}")
                    return {
                        "success": True,
                        "data": {
                            "quality": "low",
                            "reason": f"irrelevant_content_{category}",
                            "domain": domain,
                            "filter": True,  # âœ… åº”è¯¥è¿‡æ»¤
                            "score_adjustment": -10.0,
                            "matched_keywords": matched_keywords,
                            "category": category
                        },
                        "text": f"ä¸æ¨èï¼šæ˜æ˜¾æ— å…³å†…å®¹ï¼ˆ{category}ï¼‰: {', '.join(matched_keywords)}"
                    }

            # æ¸¸æˆã€ç”µå•†å…³é”®è¯ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            spam_keywords = [
                'hack', 'cheat', 'mod',
                'crack', 'patch',
                'casino', 'betting', 'gambling'
            ]

            spam_count = sum(1 for kw in spam_keywords if kw in title_lower)
            if spam_count >= 2:
                matched_keywords = [kw for kw in spam_keywords if kw in title_lower]
                logger.warning(f"[âŒ åƒåœ¾å…³é”®è¯] æ ‡é¢˜åŒ…å« {spam_count} ä¸ªåƒåœ¾å…³é”®è¯: {matched_keywords}")
                return {
                    "success": True,
                    "data": {
                        "quality": "low",
                        "reason": "spam_keywords",
                        "domain": domain,
                        "filter": True,
                        "score_adjustment": -6.0,
                        "matched_keywords": matched_keywords
                    },
                    "text": f"ä¸æ¨èï¼šæ ‡é¢˜åŒ…å«åƒåœ¾å†…å®¹å…³é”®è¯ ({domain})"
                }

        # 5. æœªçŸ¥åŸŸåï¼ˆéœ€è¦è¿›ä¸€æ­¥åˆ¤æ–­ï¼‰
        logger.info(f"[â“ æœªçŸ¥åŸŸå] åŸŸå {domain} ä¸åœ¨é»‘åå•æˆ–ä¿¡ä»»åˆ—è¡¨ä¸­ï¼Œè´¨é‡è¯„çº§: medium")
        return {
            "success": True,
            "data": {
                "quality": "medium",
                "reason": "unknown_domain",
                "domain": domain,
                "filter": False,
                "score_adjustment": 0.0,
                "matched_rule": None
            },
            "text": f"æ¥æºæœªçŸ¥ï¼š{domain}ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ¤æ–­å†…å®¹è´¨é‡"
        }

    except Exception as e:
        logger.error(f"éªŒè¯URLè´¨é‡å¤±è´¥ï¼š{str(e)}")
        return {
            "success": False,
            "error": str(e),
            "data": None,
            "text": f"éªŒè¯å¤±è´¥ï¼š{str(e)}"
        }
